# app/tasks.py

import asyncio
import os
import time
import traceback
import json
from datetime import timezone, datetime, timedelta, date
from typing import List, Dict, Any, Optional, Tuple, AsyncGenerator
import logging

import openai
from openai import OpenAIError 

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker, aliased
from sqlalchemy.future import select
from sqlalchemy import desc, func, update, cast, literal_column, nullslast, Integer as SAInteger, or_
from sqlalchemy.orm import aliased
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy import text # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è SQL –∑–∞–ø—Ä–æ—Å–∞

import telegram
from telegram.constants import ParseMode
from telegram import helpers

from telethon.errors import (
    FloodWaitError, ChannelPrivateError, UsernameInvalidError, UsernameNotOccupiedError,
    MessageIdInvalidError as TelethonMessageIdInvalidError
)
from telethon.tl.types import (
    Message, User as TelethonUserType, Channel as TelethonChannelType, Chat as TelethonChatType,
    MessageMediaPhoto, MessageMediaDocument, MessageMediaPoll, MessageMediaWebPage,
    MessageMediaGame, MessageMediaInvoice, MessageMediaGeo, MessageMediaContact, MessageMediaDice,
    MessageMediaUnsupported, MessageMediaEmpty, Poll, PollAnswer, ReactionCount, ReactionEmoji, ReactionCustomEmoji,
    MessageReplies, PeerUser, PeerChat, PeerChannel, MessageReplyHeader,
    DocumentAttributeFilename, DocumentAttributeAnimated, DocumentAttributeVideo, DocumentAttributeAudio,
    WebPage, WebPageEmpty, MessageService
)
from telethon import TelegramClient
from telethon.requestiter import RequestIter

from app.celery_app import celery_instance
from app.core.config import settings
from app.models.telegram_data import Channel, Post, Comment 
from app.db.session import get_async_session_context_manager 
from app.schemas.ui_schemas import PostRefreshMode, CommentRefreshMode 

try:
    from app.services.llm_service import –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm
except ImportError:
    async def –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm(prompt: str, –º–æ–¥–µ–ª—å: str, is_json_response_expected: bool = False, **kwargs) -> Optional[str]:
        current_logger = logging.getLogger(__name__)
        prompt_preview = prompt[:100].replace('\n', ' ')
        current_logger.warning(f"–ó–ê–ì–õ–£–®–ö–ê LLM: –ú–æ–¥–µ–ª—å='{–º–æ–¥–µ–ª—å}', JSON –æ–∂–∏–¥–∞–µ—Ç—Å—è={is_json_response_expected}. –ü—Ä–æ–º–ø—Ç (–Ω–∞—á–∞–ª–æ): '{prompt_preview}...'")

        if is_json_response_expected:
            if "–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (JSON" in prompt:
                return json.dumps({"sentiment_label": "neutral-mock", "sentiment_score": 0.0})
            elif "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–∞–º–∏" in prompt:
                return json.dumps({"topics": ["mock-topic"], "problems": [], "questions": [], "suggestions": ["mock-suggestion"]})
            else:
                return json.dumps({"message": "–ó–∞–≥–ª—É—à–∫–∞ LLM: JSON –æ—Ç–≤–µ—Ç", "data": {}})
        return f"–ó–∞–≥–ª—É—à–∫–∞ LLM: –ú–æ–¥–µ–ª—å '{–º–æ–¥–µ–ª—å}' –ø–æ–ª—É—á–∏–ª–∞ –ø—Ä–æ–º–ø—Ç (–Ω–µ JSON)."

logger = celery_instance.log.get_default_logger()

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–ë–û–†–ê –î–ê–ù–ù–´–• ---
async def _process_media_for_db(message_media: Any) -> tuple[Optional[str], Optional[Dict[str, Any]]]:
    media_type_str: Optional[str] = None; media_info_dict: Dict[str, Any] = {}
    if not message_media or isinstance(message_media, MessageMediaEmpty): return None, None
    if isinstance(message_media, MessageMediaPhoto):
        media_type_str = "photo";
        if message_media.photo and hasattr(message_media.photo, 'id'): media_info_dict['id'] = message_media.photo.id
        if hasattr(message_media, 'ttl_seconds') and message_media.ttl_seconds: media_info_dict['ttl_seconds'] = message_media.ttl_seconds
    elif isinstance(message_media, MessageMediaDocument):
        doc = message_media.document; media_type_str = "document"
        if hasattr(doc, 'id'): media_info_dict['id'] = doc.id
        if hasattr(doc, 'mime_type'): media_info_dict['mime_type'] = doc.mime_type
        filename = next((attr.file_name for attr in doc.attributes if isinstance(attr, DocumentAttributeFilename)), None)
        if filename: media_info_dict['filename'] = filename
        duration = next((attr.duration for attr in doc.attributes if isinstance(attr, (DocumentAttributeAudio, DocumentAttributeVideo))), None)
        if duration is not None: media_info_dict['duration'] = float(duration)
        is_gif = any(isinstance(attr, DocumentAttributeAnimated) for attr in doc.attributes); is_video = any(isinstance(attr, DocumentAttributeVideo) and not getattr(attr, 'round_message', False) for attr in doc.attributes); is_audio = any(isinstance(attr, DocumentAttributeAudio) and not getattr(attr, 'voice', False) for attr in doc.attributes); is_voice = any(isinstance(attr, DocumentAttributeAudio) and getattr(attr, 'voice', False) for attr in doc.attributes); is_video_note = any(isinstance(attr, DocumentAttributeVideo) and getattr(attr, 'round_message', False) for attr in doc.attributes)
        if is_gif: media_type_str = "gif"
        elif is_video_note: media_type_str = "video_note"
        elif is_voice: media_type_str = "voice"
        elif is_video: media_type_str = "video"
        elif is_audio: media_type_str = "audio"
    elif isinstance(message_media, MessageMediaPoll):
        media_type_str = "poll"; poll: Poll = message_media.poll
        media_info_dict['question'] = str(poll.question.text) if hasattr(poll.question, 'text') and poll.question.text is not None else str(poll.question)
        media_info_dict['answers'] = [{'text': str(ans.text), 'option': ans.option.decode('utf-8','replace')} for ans in poll.answers]
        media_info_dict['closed'] = poll.closed; media_info_dict['quiz'] = poll.quiz
        if message_media.results and message_media.results.total_voters is not None:
            media_info_dict['total_voters'] = message_media.results.total_voters
            if message_media.results.results: media_info_dict['results_summary'] = [{'option': ans.option.decode('utf-8','replace'), 'voters': ans.voters, 'correct': getattr(ans, 'correct', None)} for ans in message_media.results.results]
    elif isinstance(message_media, MessageMediaWebPage):
        media_type_str = "webpage"; wp: WebPage | WebPageEmpty = message_media.webpage
        if isinstance(wp, WebPage):
            media_info_dict['url'] = wp.url; media_info_dict['display_url'] = wp.display_url; media_info_dict['type'] = wp.type
            media_info_dict['site_name'] = str(wp.site_name) if wp.site_name else None
            media_info_dict['title'] = str(wp.title) if wp.title else None
            media_info_dict['description'] = str(wp.description) if wp.description else None
            if wp.photo and hasattr(wp.photo, 'id'): media_info_dict['photo_id'] = wp.photo.id
            if wp.duration: media_info_dict['duration'] = float(wp.duration)
    elif isinstance(message_media, MessageMediaGeo) and hasattr(message_media.geo, 'lat') and hasattr(message_media.geo, 'long'): media_type_str = "geo"; media_info_dict['latitude'] = message_media.geo.lat; media_info_dict['longitude'] = message_media.geo.long
    elif isinstance(message_media, MessageMediaContact): media_type_str = "contact"; media_info_dict['phone_number'] = message_media.phone_number; media_info_dict['first_name'] = message_media.first_name; media_info_dict['last_name'] = message_media.last_name
    elif isinstance(message_media, MessageMediaGame) and message_media.game: media_type_str = "game"; media_info_dict['title'] = str(message_media.game.title) if message_media.game.title else None
    elif isinstance(message_media, MessageMediaInvoice) and message_media.title: media_type_str = "invoice"; media_info_dict['title'] = str(message_media.title) if message_media.title else None
    elif isinstance(message_media, MessageMediaDice): media_type_str = "dice"; media_info_dict['emoticon'] = message_media.emoticon; media_info_dict['value'] = message_media.value
    elif isinstance(message_media, MessageMediaUnsupported): media_type_str = "unsupported"
    else:
        if media_type_str is None: media_type_str = f"other_{type(message_media).__name__}"; media_info_dict['raw_type'] = type(message_media).__name__
    return media_type_str, media_info_dict if media_info_dict else None

async def _process_reactions_for_db(message_reactions_attr: Optional[MessageReplies]) -> Optional[List[Dict[str, Any]]]:
    if not message_reactions_attr or not message_reactions_attr.results: return None
    processed_reactions = []
    for reaction_count_obj in message_reactions_attr.results:
        reaction_count_obj: ReactionCount; reaction_val = None
        if isinstance(reaction_count_obj.reaction, ReactionEmoji): reaction_val = reaction_count_obj.reaction.emoticon
        elif isinstance(reaction_count_obj.reaction, ReactionCustomEmoji): reaction_val = f"custom_emoji_{reaction_count_obj.reaction.document_id}"
        if reaction_val: processed_reactions.append({"reaction": reaction_val, "count": reaction_count_obj.count})
    return processed_reactions if processed_reactions else None


async def _helper_fetch_and_process_comments_for_post(
    tg_client: TelegramClient,
    db: AsyncSession,
    post_db_obj: Post, 
    tg_channel_entity: TelethonChannelType,
    comment_limit: int,
    log_prefix: str = "[CommentHelper]"
) -> Tuple[int, List[int]]: 
    new_comments_count_for_post = 0
    new_comment_ids_for_post: List[int] = []
    existing_comment_tg_ids_stmt = select(Comment.telegram_comment_id).where(Comment.post_id == post_db_obj.id)
    existing_comment_tg_ids_res = await db.execute(existing_comment_tg_ids_stmt)
    existing_comment_tg_ids_set = set(existing_comment_tg_ids_res.scalars().all())
    
    logger.debug(f"{log_prefix}    –ü–æ—Å—Ç ID {post_db_obj.id} (TG ID: {post_db_obj.telegram_post_id}): –≤ –ë–î —É–∂–µ –µ—Å—Ç—å {len(existing_comment_tg_ids_set)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∏–∑ Telegram (–ª–∏–º–∏—Ç: {comment_limit}).")

    flood_wait_attempts_for_post = 0
    max_flood_wait_attempts = 2 

    while flood_wait_attempts_for_post < max_flood_wait_attempts:
        try:
            message_iterator: RequestIter = tg_client.iter_messages(
                entity=tg_channel_entity,
                limit=comment_limit, 
                reply_to=post_db_obj.telegram_post_id
            )
            async for tg_comment_msg in message_iterator:
                tg_comment_msg: Message
                if tg_comment_msg.action or not (tg_comment_msg.text or tg_comment_msg.media or tg_comment_msg.poll):
                    continue
                if tg_comment_msg.id in existing_comment_tg_ids_set: 
                    continue

                comm_text, comm_caption = (None, tg_comment_msg.text) if tg_comment_msg.media and tg_comment_msg.text else (tg_comment_msg.text, None)
                comm_media_type, comm_media_info = await _process_media_for_db(tg_comment_msg.media)
                comm_reactions = await _process_reactions_for_db(tg_comment_msg.reactions)
                comm_reply_to_id = tg_comment_msg.reply_to.reply_to_msg_id if tg_comment_msg.reply_to and hasattr(tg_comment_msg.reply_to, 'reply_to_msg_id') else None
                comm_user_id, comm_user_username, comm_user_fullname = (None, None, None)
                if isinstance(tg_comment_msg.sender, TelethonUserType):
                    comm_user_id = tg_comment_msg.sender.id; comm_user_username = tg_comment_msg.sender.username
                    comm_user_fullname = f"{tg_comment_msg.sender.first_name or ''} {tg_comment_msg.sender.last_name or ''}".strip() or None
                elif tg_comment_msg.from_id and isinstance(tg_comment_msg.from_id, PeerUser):
                    comm_user_id = tg_comment_msg.from_id.user_id

                new_comment_db = Comment(
                    telegram_comment_id=tg_comment_msg.id, post_id=post_db_obj.id,
                    telegram_user_id=comm_user_id, user_username=comm_user_username, user_fullname=comm_user_fullname,
                    text_content=comm_text or (comm_caption if not comm_text else ""),
                    commented_at=tg_comment_msg.date.replace(tzinfo=timezone.utc) if tg_comment_msg.date else datetime.now(timezone.utc),
                    reactions=comm_reactions, reply_to_telegram_comment_id=comm_reply_to_id,
                    media_type=comm_media_type, media_content_info=comm_media_info,
                    caption_text=comm_caption,
                    edited_at=tg_comment_msg.edit_date.replace(tzinfo=timezone.utc) if tg_comment_msg.edit_date else None,
                )
                db.add(new_comment_db)
                await db.flush() 
                if new_comment_db.id: new_comment_ids_for_post.append(new_comment_db.id)
                new_comments_count_for_post += 1
                existing_comment_tg_ids_set.add(tg_comment_msg.id) 

            if new_comments_count_for_post > 0:
                logger.info(f"{log_prefix}    –î–ª—è –ø–æ—Å—Ç–∞ ID {post_db_obj.id} (TG ID: {post_db_obj.telegram_post_id}) –¥–æ–±–∞–≤–ª–µ–Ω–æ {new_comments_count_for_post} –Ω–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –ë–î.")
            
            break 

        except TelethonMessageIdInvalidError:
            logger.warning(f"{log_prefix}    –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.telegram_post_id} (DB ID: {post_db_obj.id}) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –≤ Telegram (MsgIdInvalid). –†–∞–Ω–µ–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤ –ë–î: {len(existing_comment_tg_ids_set)}.")
            break
        except FloodWaitError as fwe_c:
            flood_wait_attempts_for_post += 1
            if flood_wait_attempts_for_post >= max_flood_wait_attempts:
                logger.error(f"{log_prefix}    –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç ({max_flood_wait_attempts}) –ø–æ–ø—ã—Ç–æ–∫ FloodWait –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.telegram_post_id}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä –∫–æ–º–º. –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ.")
                break
            else:
                logger.warning(f"{log_prefix}    FloodWait ({fwe_c.seconds}s) –ø—Ä–∏ —Å–±–æ—Ä–µ –∫–æ–º–º. –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.telegram_post_id} (–ø–æ–ø—ã—Ç–∫–∞ {flood_wait_attempts_for_post}/{max_flood_wait_attempts}). –ñ–¥–µ–º –∏ –ø—ã—Ç–∞–µ–º—Å—è —Å–Ω–æ–≤–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∂–µ –ø–æ—Å—Ç–∞.")
                await asyncio.sleep(fwe_c.seconds + 5) 
        except Exception as e_c:
            logger.error(f"{log_prefix}    –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∫–æ–º–º. –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.telegram_post_id} (DB ID: {post_db_obj.id}): {type(e_c).__name__} - {e_c}", exc_info=True)
            break
            
    return new_comments_count_for_post, new_comment_ids_for_post


async def _helper_fetch_and_process_posts_for_channel(
    tg_client: TelegramClient,
    db: AsyncSession,
    channel_db: Channel,
    iter_params: Dict[str, Any],
    update_existing_info_flag: bool = False,
    log_prefix: str = "[PostHelper]"
) -> Tuple[List[Post], List[Post], int, int, int]:
    posts_for_comment_scan_candidates: List[Post] = []
    newly_created_post_objects: List[Post] = []
    new_posts_count_channel = 0
    updated_posts_count_channel = 0
    latest_post_id_tg_seen_this_run = channel_db.last_processed_post_id or 0

    message_iterator: RequestIter = tg_client.iter_messages(**iter_params)
    async for tg_message in message_iterator:
        tg_message: Message
        if isinstance(tg_message, MessageService) or tg_message.action: continue
        if not (tg_message.text or tg_message.media or tg_message.poll): continue
        
        if tg_message.id > latest_post_id_tg_seen_this_run:
            latest_post_id_tg_seen_this_run = tg_message.id
            
        existing_post_stmt = select(Post).where(Post.telegram_post_id == tg_message.id, Post.channel_id == channel_db.id)
        existing_post_db = (await db.execute(existing_post_stmt)).scalar_one_or_none()
        
        post_text_content, post_caption_text = (None, tg_message.text) if tg_message.media and tg_message.text else (tg_message.text if not tg_message.media else None, None)
        media_type, media_info = await _process_media_for_db(tg_message.media)
        reactions_data = await _process_reactions_for_db(tg_message.reactions)
        reply_to_id = tg_message.reply_to.reply_to_msg_id if tg_message.reply_to and hasattr(tg_message.reply_to, 'reply_to_msg_id') else None
        sender_id_val = tg_message.from_id.user_id if isinstance(tg_message.from_id, PeerUser) else None
        link_val = f"https://t.me/{channel_db.username or f'c/{channel_db.id}'}/{tg_message.id}"
        posted_at_val = tg_message.date.replace(tzinfo=timezone.utc) if tg_message.date else datetime.now(timezone.utc)
        edited_at_val = tg_message.edit_date.replace(tzinfo=timezone.utc) if tg_message.edit_date else None
        
        api_comments_count = tg_message.replies.replies if tg_message.replies and tg_message.replies.replies is not None else 0

        post_for_comments_scan_candidate = None
        if not existing_post_db:
            new_post = Post(
                telegram_post_id=tg_message.id, channel_id=channel_db.id, link=link_val,
                text_content=post_text_content, caption_text=post_caption_text,
                views_count=tg_message.views, comments_count=api_comments_count, 
                posted_at=posted_at_val, reactions=reactions_data, media_type=media_type,
                media_content_info=media_info, reply_to_telegram_post_id=reply_to_id,
                forwards_count=tg_message.forwards, author_signature=tg_message.post_author,
                sender_user_id=sender_id_val, grouped_id=tg_message.grouped_id,
                edited_at=edited_at_val, is_pinned=tg_message.pinned or False
            )
            db.add(new_post); await db.flush(); post_for_comments_scan_candidate = new_post; new_posts_count_channel +=1
            newly_created_post_objects.append(new_post)
        elif update_existing_info_flag:
            existing_post_db.views_count = tg_message.views
            existing_post_db.reactions = reactions_data
            existing_post_db.forwards_count = tg_message.forwards
            existing_post_db.edited_at = edited_at_val
            existing_post_db.comments_count = api_comments_count 
            existing_post_db.text_content = post_text_content
            existing_post_db.caption_text = post_caption_text
            existing_post_db.media_type = media_type
            existing_post_db.media_content_info = media_info
            existing_post_db.is_pinned = tg_message.pinned or False
            existing_post_db.author_signature = tg_message.post_author
            existing_post_db.updated_at = datetime.now(timezone.utc)
            
            db.add(existing_post_db); post_for_comments_scan_candidate = existing_post_db; updated_posts_count_channel += 1
        else: 
            post_for_comments_scan_candidate = existing_post_db
            if existing_post_db.comments_count != api_comments_count:
                logger.debug(f"{log_prefix} –ü–æ—Å—Ç TG ID {existing_post_db.telegram_post_id}: comments_count –æ–±–Ω–æ–≤–ª–µ–Ω —Å {existing_post_db.comments_count} –Ω–∞ {api_comments_count} (update_existing_info_flag=False).")
                existing_post_db.comments_count = api_comments_count
                existing_post_db.updated_at = datetime.now(timezone.utc)
                db.add(existing_post_db)

        if post_for_comments_scan_candidate: posts_for_comment_scan_candidates.append(post_for_comments_scan_candidate)

    return posts_for_comment_scan_candidates, newly_created_post_objects, new_posts_count_channel, updated_posts_count_channel, latest_post_id_tg_seen_this_run

# --- –ó–ê–î–ê–ß–ò CELERY ---

@celery_instance.task(name="add")
def add(x, y):
    logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–∞—Å–∫ 'add': {x} + {y}")
    time.sleep(5)
    result = x + y
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç 'add': {result}")
    return result

@celery_instance.task(name="simple_debug_task")
def simple_debug_task(message: str):
    logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–∞—Å–∫ 'simple_debug_task' –ø–æ–ª—É—á–∏–ª: {message}")
    time.sleep(3)
    return f"–°–æ–æ–±—â–µ–Ω–∏–µ '{message}' –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"

@celery_instance.task(name="collect_telegram_data", bind=True, max_retries=3, default_retry_delay=60 * 5)
def collect_telegram_data_task(self):
    task_start_time = time.time(); log_prefix = "[CollectDataTask]"; logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id})...")
    api_id_val = settings.TELEGRAM_API_ID; api_hash_val = settings.TELEGRAM_API_HASH; phone_number_val = settings.TELEGRAM_PHONE_NUMBER_FOR_LOGIN
    if not all([api_id_val, api_hash_val, phone_number_val]):
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: Telegram API credentials –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã.")
        return "Config error: Telegram API credentials"
    session_file_path = "/app/celery_telegram_session"

    async def _async_collect_data_logic():
        tg_client = None; total_ch_proc, total_new_p, total_upd_p, total_new_c = 0,0,0,0
        all_new_comment_ids_task_total: List[int] = []
        local_engine = None
        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL 
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)
            
            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏ {self.name}.")

            async with LocalAsyncSessionFactory_Task() as db: 
                tg_client = TelegramClient(session_file_path, api_id_val, api_hash_val)
                await tg_client.connect()
                if not await tg_client.is_user_authorized():
                    raise ConnectionRefusedError(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω –¥–ª—è {session_file_path}.session")
                me = await tg_client.get_me()
                logger.info(f"{log_prefix} TGClient –ø–æ–¥–∫–ª—é—á–µ–Ω –∫–∞–∫: {me.first_name if me else 'N/A'}")
                active_channels_db_result = await db.execute(select(Channel).where(Channel.is_active == True))
                active_channels_db: List[Channel] = active_channels_db_result.scalars().all()

                if not active_channels_db:
                    logger.info(f"{log_prefix} –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.")
                    return "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤."

                for idx, channel_db in enumerate(active_channels_db):
                    total_ch_proc +=1
                    logger.info(f"{log_prefix} –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: {channel_db.title} (ID: {channel_db.id}) ({idx+1}/{len(active_channels_db)})")
                    try:
                        tg_channel_entity = await tg_client.get_entity(channel_db.id)
                        if not isinstance(tg_channel_entity, TelethonChannelType) or not (getattr(tg_channel_entity, 'broadcast', False) or getattr(tg_channel_entity, 'megagroup', False)):
                            logger.warning(f"{log_prefix}  –ö–∞–Ω–∞–ª {channel_db.id} –Ω–µ–≤–∞–ª–∏–¥–µ–Ω. –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º.")
                            channel_db.is_active = False; db.add(channel_db)
                            continue
                        iter_params: Dict[str, Any] = {"entity": tg_channel_entity, "limit": settings.POST_FETCH_LIMIT}
                        if channel_db.last_processed_post_id:
                            iter_params["min_id"] = channel_db.last_processed_post_id
                        elif settings.INITIAL_POST_FETCH_START_DATETIME:
                            iter_params["offset_date"] = settings.INITIAL_POST_FETCH_START_DATETIME
                            iter_params["reverse"] = True

                        # update_existing_info_flag=False, —Ç.–∫. —ç—Ç–æ —Å–±–æ—Ä —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
                        processed_posts_list, newly_created_posts, new_p_ch, _, last_id_tg = await _helper_fetch_and_process_posts_for_channel(
                            tg_client, db, channel_db, iter_params, 
                            update_existing_info_flag=False, 
                            log_prefix=log_prefix
                        )
                        total_new_p += new_p_ch
                        if last_id_tg > (channel_db.last_processed_post_id or 0):
                            channel_db.last_processed_post_id = last_id_tg
                            db.add(channel_db)

                        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ù–û–í–´–• –ø–æ—Å—Ç–æ–≤
                        if newly_created_posts:
                            logger.info(f"{log_prefix}  –°–±–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è {len(newly_created_posts)} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤...")
                            for post_obj in newly_created_posts:
                                num_c, new_c_ids = await _helper_fetch_and_process_comments_for_post(
                                    tg_client, db, post_obj, tg_channel_entity, 
                                    settings.COMMENT_FETCH_LIMIT, log_prefix=log_prefix
                                )
                                total_new_c += num_c
                                all_new_comment_ids_task_total.extend(new_c_ids)
                                # –ü–æ—Å–ª–µ —Å–±–æ—Ä–∞ –∫–æ–º–º–µ–Ω—Ç–æ–≤, Post.comments_count –≤ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ post_obj –ù–ï –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∑–¥–µ—Å—å,
                                # —Ç.–∫. _helper_fetch_and_process_comments_for_post –±–æ–ª—å—à–µ –Ω–µ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ–≥–æ.
                                # Post.comments_count –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑ API –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–æ—Å—Ç–∞ –≤ _helper_fetch_and_process_posts_for_channel.
                                # –≠—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–ª—è –ù–û–í–´–• –ø–æ—Å—Ç–æ–≤.

                    except (ChannelPrivateError, UsernameInvalidError, UsernameNotOccupiedError) as e_ch_access:
                        logger.warning(f"{log_prefix}  –ö–∞–Ω–∞–ª {channel_db.id} ('{channel_db.title}') –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e_ch_access}. –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º.")
                        channel_db.is_active = False; db.add(channel_db)
                        continue
                    except FloodWaitError as fwe_ch:
                        logger.warning(f"{log_prefix}  FloodWait ({fwe_ch.seconds} —Å–µ–∫.) –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db.title}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                        await asyncio.sleep(fwe_ch.seconds + 10)
                        continue
                    except Exception as e_ch_proc:
                        logger.error(f"{log_prefix}  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ '{channel_db.title}': {type(e_ch_proc).__name__} - {e_ch_proc}", exc_info=True)
                        continue

                    if idx < len(active_channels_db) - 1:
                        logger.debug(f"{log_prefix} –ü–∞—É–∑–∞ 1 —Å–µ–∫ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–Ω–∞–ª–∞.")
                        await asyncio.sleep(1)
                await db.commit()
                summary = f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω. –ö–∞–Ω–∞–ª–æ–≤: {total_ch_proc}, –ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤: {total_new_p}, –ù–æ–≤—ã—Ö –∫–æ–º–º. —Å–æ–±—Ä–∞–Ω–æ (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤): {total_new_c}."
                logger.info(f"{log_prefix} {summary}")
                
                if all_new_comment_ids_task_total:
                    unique_comment_ids_for_ai = sorted(list(set(all_new_comment_ids_task_total)))
                    logger.info(f"{log_prefix} –ó–∞–ø—É—Å–∫ AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(unique_comment_ids_for_ai)} –Ω–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ collect_telegram_data_task.")
                    
                    ai_analysis_sub_batch_size = settings.COMMENT_ENQUEUE_BATCH_SIZE 
                    num_sub_tasks = 0
                    for i in range(0, len(unique_comment_ids_for_ai), ai_analysis_sub_batch_size):
                        batch_comment_ids = unique_comment_ids_for_ai[i:i + ai_analysis_sub_batch_size]
                        logger.info(f"{log_prefix}  –°—Ç–∞–≤–ª—é –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ AI-–∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á—É enqueue_comments_for_ai_feature_analysis_task —Å {len(batch_comment_ids)} ID –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.")
                        enqueue_comments_for_ai_feature_analysis_task.delay(
                            comment_ids_to_process=batch_comment_ids
                        )
                        num_sub_tasks += 1
                    logger.info(f"{log_prefix} AI-–∞–Ω–∞–ª–∏–∑ –¥–ª—è {len(unique_comment_ids_for_ai)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å ({num_sub_tasks} –∑–∞–¥–∞—á(–∏) enqueue_comments_for_ai_feature_analysis_task).")

                return summary
        except ConnectionRefusedError as e_auth:
            logger.error(f"{log_prefix} –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò TELETHON: {e_auth}", exc_info=True); raise
        except Exception as e_main:
            logger.error(f"{log_prefix} –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {type(e_main).__name__} - {e_main}", exc_info=True); raise
        finally:
            if tg_client and tg_client.is_connected():
                await tg_client.disconnect()
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally –¥–ª—è –∑–∞–¥–∞—á–∏ {self.name}.")
                await local_engine.dispose()
    try:
        result = asyncio.run(_async_collect_data_logic())
        logger.info(f"{log_prefix} –¢–∞—Å–∫ '{self.name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {time.time() - task_start_time:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return result
    except ConnectionRefusedError as e_final_auth:
        logger.error(f"{log_prefix} –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò (–Ω–µ —Ä–µ—Ç—Ä–∞–∏–º): {e_final_auth}", exc_info=True)
        raise e_final_auth
    except Exception as e_final_task:
        logger.error(f"{log_prefix} –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ó–ê–î–ê–ß–ò (—Ä–µ—Ç—Ä–∞–π): {type(e_final_task).__name__} - {e_final_task}", exc_info=True)
        try:
            if self.request.retries < self.max_retries:
                default_retry_delay_val = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 300
                countdown = int(default_retry_delay_val * (2 ** self.request.retries))
                logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫")
                raise self.retry(exc=e_final_task, countdown=countdown)
            else:
                logger.error(f"Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}.")
                raise e_final_task
        except Exception as e_retry_logic:
            logger.error(f"Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}: {type(e_retry_logic).__name__}", exc_info=True)
            raise e_final_task

@celery_instance.task(name="summarize_posts_batch", bind=True, max_retries=2, default_retry_delay=300)
def summarize_posts_batch_task(
    self,
    channel_ids: Optional[List[int]] = None,
    start_date_iso: Optional[str] = None,
    end_date_iso: Optional[str] = None,
):
    limit_posts_to_process = settings.POST_SUMMARY_BATCH_SIZE
    task_start_time = time.time()
    log_prefix_parts = ["[PostSummaryTask"]
    if channel_ids: log_prefix_parts.append(f"Channels:{channel_ids}")
    if start_date_iso: log_prefix_parts.append(f"Start:{start_date_iso}")
    if end_date_iso: log_prefix_parts.append(f"End:{end_date_iso}")
    log_prefix = "".join(log_prefix_parts) + "]"

    logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}). –õ–∏–º–∏—Ç –ø–∞—á–∫–∏: {limit_posts_to_process}.")

    if not settings.OPENAI_API_KEY:
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.")
        self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏', 'error': 'OpenAI API Key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.', 'progress': 0, 'processed_count':0, 'total_to_process':0})
        return "Config error: OpenAI Key not configured." # –≠—Ç–æ—Ç return –≤—Å–µ –µ—â–µ –∞–∫—Ç—É–∞–ª–µ–Ω, —Ç–∞–∫ –∫–∞–∫ –∑–∞–¥–∞—á–∞ –Ω–µ –º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å—Å—è

    progress_info_ref: Dict[str, Any] = {
        "processed_count": 0,
        "total_to_process": 0,
        "current_step": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏",
        "progress": 0
    }

    async def _async_logic(task_instance, current_progress_info_ref: Dict[str, Any]):
        processed_count_in_batch = 0 # –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–Ω—ã—Ö –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        total_posts_for_batch = 0
        local_engine = None

        current_progress_info_ref.update({
            'current_step': '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤',
            'progress': 5,
            'processed_count': 0,
            'total_to_process': 0
        })
        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)

            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏.")

            async with LocalAsyncSessionFactory_Task() as db_session:
                stmt = (
                    select(Post)
                    .where(Post.summary_text.is_(None))
                    .where(or_(Post.text_content.isnot(None), Post.caption_text.isnot(None))) # –ï—Å—Ç—å —á—Ç–æ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                    .order_by(Post.posted_at.asc())
                    .limit(limit_posts_to_process)
                )

                if channel_ids:
                    logger.info(f"{log_prefix} –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤: {channel_ids}")
                    stmt = stmt.where(Post.channel_id.in_(channel_ids))
                else:
                    logger.info(f"{log_prefix} –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (–µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã).")
                    if not start_date_iso and not end_date_iso:
                         active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery()
                         stmt = stmt.join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id)

                if start_date_iso:
                    dt_start_naive = date.fromisoformat(start_date_iso.split('T')[0])
                    dt_start = datetime(dt_start_naive.year, dt_start_naive.month, dt_start_naive.day, 0, 0, 0, 0, tzinfo=timezone.utc)
                    stmt = stmt.where(Post.posted_at >= dt_start)
                    logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä start_date: {dt_start}")
                if end_date_iso:
                    dt_end_naive = date.fromisoformat(end_date_iso.split('T')[0])
                    dt_end = datetime(dt_end_naive.year, dt_end_naive.month, dt_end_naive.day, 23, 59, 59, 999999, tzinfo=timezone.utc)
                    stmt = stmt.where(Post.posted_at <= dt_end)
                    logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä end_date: {dt_end}")

                posts_to_process_result = await db_session.execute(stmt)
                posts_to_process = posts_to_process_result.scalars().all()
                total_posts_for_batch = len(posts_to_process)
                current_progress_info_ref['total_to_process'] = total_posts_for_batch

                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ.")
                current_progress_info_ref.update({
                    'current_step': f'–ù–∞–π–¥–µ–Ω–æ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏',
                    'progress': 10
                })
                task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

                if not posts_to_process:
                    logger.info(f"{log_prefix} –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–µ.")
                    result_message = "–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ."
                    current_progress_info_ref.update({
                        'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ: –Ω–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏',
                        'progress': 100,
                        'result_summary': result_message
                    })
                    task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
                    return result_message

                for i, post_obj in enumerate(posts_to_process):
                    text_to_summarize = post_obj.caption_text if post_obj.caption_text and post_obj.caption_text.strip() else post_obj.text_content
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å—Ç–æ–≤ –∏–ª–∏ –ø–æ—Å—Ç–æ–≤ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
                    # –Ω–æ –ø–æ–º–µ—á–∞–µ–º –∏—Ö –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (—Å –ø—É—Å—Ç—ã–º —Ä–µ–∑—é–º–µ), —á—Ç–æ–±—ã –Ω–µ –≤—ã–±–∏—Ä–∞—Ç—å —Å–Ω–æ–≤–∞
                    if not text_to_summarize or len(text_to_summarize.strip()) < (settings.MIN_POST_LENGTH_FOR_SUMMARY or 30):
                        logger.info(f"{log_prefix}  –ü–æ—Å—Ç ID {post_obj.id} ({post_obj.link}) —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞. –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞, –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–ø—É—Å—Ç—ã–º —Ä–µ–∑—é–º–µ).")
                        post_obj.summary_text = "" # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ø–æ—Å—Ç –±—ã–ª —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω, –Ω–æ –Ω–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω
                        post_obj.updated_at = datetime.now(timezone.utc)
                        db_session.add(post_obj)
                        processed_count_in_batch += 1
                        current_progress_info_ref['processed_count'] = processed_count_in_batch
                    else:
                        logger.info(f"{log_prefix}  –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–∞ ID {post_obj.id} ({post_obj.link})...")
                        try:
                            summary_prompt = f"–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:\n---\n{text_to_summarize[:settings.LLM_MAX_PROMPT_LENGTH]}\n---\n–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º) –æ—Å–Ω–æ–≤–Ω–æ–π –º—ã—Å–ª–∏ —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞."
                            summary = await –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm(
                                summary_prompt,
                                –º–æ–¥–µ–ª—å=settings.OPENAI_DEFAULT_MODEL_FOR_TASKS or "gpt-3.5-turbo",
                                —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞=0.3,
                                –º–∞–∫—Å_—Ç–æ–∫–µ–Ω—ã=settings.LLM_SUMMARY_MAX_TOKENS or 250, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç
                                is_json_response_expected=False
                            )
                            if summary and summary.strip():
                                post_obj.summary_text = summary.strip()
                                post_obj.updated_at = datetime.now(timezone.utc)
                                db_session.add(post_obj)
                                processed_count_in_batch += 1
                                current_progress_info_ref['processed_count'] = processed_count_in_batch
                                logger.info(f"{log_prefix}    –†–µ–∑—é–º–µ –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id} –ø–æ–ª—É—á–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ.")
                            else:
                                logger.warning(f"{log_prefix}    LLM –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}. –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (–ø—É—Å—Ç—ã–º —Ä–µ–∑—é–º–µ).")
                                post_obj.summary_text = "" # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                                post_obj.updated_at = datetime.now(timezone.utc)
                                db_session.add(post_obj)
                                processed_count_in_batch += 1 # –í—Å–µ —Ä–∞–≤–Ω–æ —Å—á–∏—Ç–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º
                                current_progress_info_ref['processed_count'] = processed_count_in_batch

                        except OpenAIError as e_llm:
                            logger.error(f"{log_prefix}    !!! –û—à–∏–±–∫–∞ OpenAI API –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {type(e_llm).__name__} - {e_llm}")
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –ø–æ—Å—Ç, –æ–Ω –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –±–µ–∑ —Ä–µ–∑—é–º–µ –¥–ª—è —ç—Ç–æ–π –ø–æ–ø—ã—Ç–∫–∏
                            continue
                        except Exception as e_sum:
                            logger.error(f"{log_prefix}    !!! –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {type(e_sum).__name__} - {e_sum}", exc_info=True)
                            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –ø–æ—Å—Ç

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
                    if (i + 1) % 10 == 0 or (i + 1) == total_posts_for_batch:
                        current_progress_percentage = 10 + int(((i + 1) / total_posts_for_batch) * 85) if total_posts_for_batch > 0 else 95
                        current_progress_info_ref.update({
                            'current_step': f'–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count_in_batch}/{total_posts_for_batch} (–ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ {i+1})',
                            'progress': current_progress_percentage
                        })
                        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)
                
                # –ö–æ–º–º–∏—Ç, –µ—Å–ª–∏ –±—ã–ª–∏ –∫–∞–∫–∏–µ-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è (—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ/–∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å—Ç—ã)
                if processed_count_in_batch > 0: # processed_count_in_batch –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö (—É—Å–ø–µ—Ö LLM –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ)
                    await db_session.commit()
                    logger.info(f"{log_prefix}  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ (—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ/–ø—Ä–æ–ø—É—â–µ–Ω–æ) {processed_count_in_batch} –ø–æ—Å—Ç–æ–≤ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ.")

            result_message = f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø–∞—á–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ (—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ/–ø—Ä–æ–ø—É—â–µ–Ω–æ): {processed_count_in_batch} –∏–∑ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤."
            current_progress_info_ref.update({
                'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ',
                'progress': 100,
                'result_summary': result_message
            })
            task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
            return result_message
        except Exception as e_async_main:
            logger.error(f"{log_prefix} !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_logic: {type(e_async_main).__name__} - {e_async_main}", exc_info=True)
            current_progress_info_ref.update({
                'current_step': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ async_logic: {type(e_async_main).__name__}',
                'error': str(e_async_main)
            })
            raise
        finally:
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally.")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_logic(self, progress_info_ref))
        task_duration = time.time() - task_start_time
        logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        return result_message
    except Exception as e_task_main:
        task_duration = time.time() - task_start_time
        logger.error(f"{log_prefix} !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_main).__name__} - {e_task_main}", exc_info=True)

        failure_meta = {
            'current_step': progress_info_ref.get('current_step', '–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏'),
            'error': str(e_task_main),
            'progress': progress_info_ref.get('progress', 0),
            'processed_count': progress_info_ref.get('processed_count', 0),
            'total_to_process': progress_info_ref.get('total_to_process', 0)
        }
        failure_meta['progress'] = 100
        failure_meta['current_step'] = f"–û—à–∏–±–∫–∞: {failure_meta['current_step']}"

        self.update_state(state='FAILURE', meta=failure_meta)

        try:
            if self.request.retries < self.max_retries:
                logger.info(f"{log_prefix} –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–¥–∞—á–∏ {self.request.id} ({self.request.retries + 1}/{self.max_retries}). –ó–∞–¥–µ—Ä–∂–∫–∞: {int(self.default_retry_delay * (2 ** self.request.retries))} —Å–µ–∫.")
                raise self.retry(exc=e_task_main, countdown=int(self.default_retry_delay * (2 ** self.request.retries)))
            else:
                logger.error(f"{log_prefix} Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}.")
                # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ e_task_main "–≤—ã–ø–∞–¥–µ—Ç", Celery –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –µ–≥–æ.
        except Exception as e_retry_logic:
            logger.error(f"{log_prefix} Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}: {type(e_retry_logic).__name__}", exc_info=True)
            self.update_state(state='FAILURE', meta={**failure_meta, 'error': f"Retry logic failed: {str(e_retry_logic)}. Original error: {str(e_task_main)}"})
            raise

@celery_instance.task(name="send_daily_digest", bind=True, max_retries=3, default_retry_delay=180)
def send_daily_digest_task(self, hours_ago_posts=24, top_n_metrics=3):
    task_start_time = time.time(); log_prefix = "[DigestTask]"; logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id})...")
    if not settings.TELEGRAM_BOT_TOKEN or not settings.TELEGRAM_TARGET_CHAT_ID:
        error_msg = "–û—à–∏–±–∫–∞: TELEGRAM_BOT_TOKEN –∏–ª–∏ TELEGRAM_TARGET_CHAT_ID –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã."
        logger.error(error_msg)
        return error_msg
        
    local_engine = None

    async def _async_send_digest_logic():
        nonlocal local_engine
        bot = telegram.Bot(token=settings.TELEGRAM_BOT_TOKEN); message_parts = []
        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL 
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)
            
            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏ {self.name}.")

            async with LocalAsyncSessionFactory_Task() as db_session:
                time_threshold_posts = datetime.now(timezone.utc) - timedelta(hours=hours_ago_posts)
                message_parts.append(helpers.escape_markdown(f" digest for Insight-Compass –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours_ago_posts} —á–∞—Å–∞:\n", version=2))
                active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery("active_channels_sq_digest")
                stmt_new_posts_count = (select(func.count(Post.id)).join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id).where(Post.posted_at >= time_threshold_posts))
                new_posts_count = (await db_session.execute(stmt_new_posts_count)).scalar_one_or_none() or 0
                message_parts.append(helpers.escape_markdown(f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ (–∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤): {new_posts_count}\n", version=2))

                async def get_top_posts_by_metric(metric_column_or_expr_name: str, metric_display_name: str, is_sum_from_jsonb_flag=False):
                    top_posts_list = []; p_alias = aliased(Post, name=f"p_digest_{''.join(filter(str.isalnum, metric_display_name.lower()))}")
                    select_fields = [p_alias.link, p_alias.summary_text, p_alias.post_sentiment_label, Channel.title.label("channel_title")]; stmt_top = select(*select_fields); order_by_col = None
                    if is_sum_from_jsonb_flag:
                        rx_elements_cte = (select(p_alias.id.label("post_id_for_reactions"), cast(func.jsonb_array_elements(p_alias.reactions).op('->>')('count'), SAInteger).label("reaction_item_count")).select_from(p_alias).where(p_alias.reactions.isnot(None)).where(func.jsonb_typeof(p_alias.reactions) == 'array').cte(f"rx_elements_cte_{''.join(filter(str.isalnum, metric_display_name.lower()))}"))
                        sum_reactions_cte = (select(rx_elements_cte.c.post_id_for_reactions.label("post_id"), func.sum(rx_elements_cte.c.reaction_item_count).label("metric_value_for_digest")).group_by(rx_elements_cte.c.post_id_for_reactions).cte(f"sum_rx_cte_{''.join(filter(str.isalnum, metric_display_name.lower()))}"))
                        stmt_top = stmt_top.add_columns(sum_reactions_cte.c.metric_value_for_digest).outerjoin(sum_reactions_cte, p_alias.id == sum_reactions_cte.c.post_id); order_by_col = sum_reactions_cte.c.metric_value_for_digest
                    else:
                        actual_metric_column = getattr(p_alias, metric_column_or_expr_name)
                        stmt_top = stmt_top.add_columns(actual_metric_column.label("metric_value_for_digest")); order_by_col = actual_metric_column
                    stmt_top = stmt_top.join(Channel, p_alias.channel_id == Channel.id).where(Channel.is_active == True).where(p_alias.posted_at >= time_threshold_posts).where(p_alias.summary_text.isnot(None)).order_by(order_by_col.desc().nullslast()).limit(top_n_metrics)
                    for row in (await db_session.execute(stmt_top)).all():
                        top_posts_list.append({"link": row.link, "summary": row.summary_text, "sentiment": row.post_sentiment_label, "channel_title": row.channel_title, "metric_value": row.metric_value_for_digest})
                    return top_posts_list

                tops_to_include = [
                    {"metric_name": "comments_count", "label": "üí¨ –¢–æ–ø –ø–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º", "emoji": "üó£Ô∏è", "unit": "–ö–æ–º–º."},
                    {"metric_name": "views_count", "label": "üëÄ –¢–æ–ø –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º", "emoji": "üëÅÔ∏è", "unit": "–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"},
                    {"metric_name": "reactions", "label": "‚ù§Ô∏è –¢–æ–ø –ø–æ —Å—É–º–º–µ —Ä–µ–∞–∫—Ü–∏–π", "emoji": "üëç", "unit": "–†–µ–∞–∫—Ü–∏–π", "is_sum_jsonb": True}
                ]
                for top_conf in tops_to_include:
                    posts_data = await get_top_posts_by_metric(top_conf["metric_name"], top_conf["label"], is_sum_from_jsonb_flag=top_conf.get("is_sum_jsonb", False))
                    if posts_data:
                        message_parts.append(f"\n{helpers.escape_markdown(top_conf['label'], version=2)} \\(—Å AI\\-—Ä–µ–∑—é–º–µ, —Ç–æ–ø\\-{len(posts_data)}\\):\n")
                        for i, p_data in enumerate(posts_data):
                            link_md = helpers.escape_markdown(p_data["link"] or "#", version=2)
                            summary_md = helpers.escape_markdown(p_data["summary"] or "–†–µ–∑—é–º–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.", version=2)
                            channel_md = helpers.escape_markdown(p_data["channel_title"] or "–ù–µ–∏–∑–≤. –∫–∞–Ω–∞–ª", version=2)
                            metric_val_md = helpers.escape_markdown(str(p_data["metric_value"] or 0), version=2)
                            s_emoji = "üòê "; s_label_text = helpers.escape_markdown("N/A", version=2)
                            if p_data["sentiment"]:
                                s_label_text = helpers.escape_markdown(p_data["sentiment"].capitalize(), version=2)
                                if p_data["sentiment"] == "positive":
                                    s_emoji = "üòä "
                                elif p_data["sentiment"] == "negative":
                                    s_emoji = "üò† "
                            message_parts.append(f"\n{i+1}\\. {channel_md} [{helpers.escape_markdown('–ü–æ—Å—Ç',version=2)}]({link_md})\n   {top_conf['emoji']} {helpers.escape_markdown(top_conf['unit'], version=2)}: {metric_val_md} {s_emoji}{s_label_text}\n   üìù _{summary_md}_\n")
            digest_message_final = "".join(message_parts)
            if len(digest_message_final) > 4096:
                digest_message_final = digest_message_final[:4090] + helpers.escape_markdown("...", version=2)
                logger.warning("  –í–ù–ò–ú–ê–ù–ò–ï: –î–∞–π–¥–∂–µ—Å—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω.")
            logger.info(f"  –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞ (–Ω–∞—á–∞–ª–æ):\n---\n{digest_message_final[:500]}...\n---")
            await bot.send_message(chat_id=settings.TELEGRAM_TARGET_CHAT_ID, text=digest_message_final, parse_mode=ParseMode.MARKDOWN_V2, disable_web_page_preview=True)
            return f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω."
        except Exception as e_digest_logic:
            logger.error(f"!!! –û—à–∏–±–∫–∞ –≤ _async_send_digest_logic: {type(e_digest_logic).__name__} - {e_digest_logic}", exc_info=True)
            raise
        finally:
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally –¥–ª—è –∑–∞–¥–∞—á–∏ {self.name}.")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_send_digest_logic()); task_duration = time.time() - task_start_time; logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except Exception as e_task_digest:
        task_duration = time.time() - task_start_time; logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_digest).__name__} - {e_task_digest}", exc_info=True)
        try:
            if self.request.retries < self.max_retries:
                default_retry_delay_val = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 180
                countdown = int(default_retry_delay_val * (2 ** self.request.retries))
                logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫")
                raise self.retry(exc=e_task_digest, countdown=countdown)
            else:
                logger.error(f"Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}.")
                raise e_task_digest
        except Exception as e_retry_logic:
            logger.error(f"Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}: {type(e_retry_logic).__name__}", exc_info=True)
            raise e_retry_logic

@celery_instance.task(name="analyze_posts_sentiment", bind=True, max_retries=2, default_retry_delay=300)
def analyze_posts_sentiment_task(
    self,
    channel_ids: Optional[List[int]] = None,
    start_date_iso: Optional[str] = None,
    end_date_iso: Optional[str] = None,
):
    limit_posts_to_process = settings.POST_ANALYSIS_BATCH_SIZE
    task_start_time = time.time()
    log_prefix_parts = ["[PostSentimentTask"]
    if channel_ids: log_prefix_parts.append(f"Channels:{channel_ids}")
    if start_date_iso: log_prefix_parts.append(f"Start:{start_date_iso}")
    if end_date_iso: log_prefix_parts.append(f"End:{end_date_iso}")
    log_prefix = "".join(log_prefix_parts) + "]"

    logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}). –õ–∏–º–∏—Ç –ø–∞—á–∫–∏: {limit_posts_to_process}.")

    if not settings.OPENAI_API_KEY:
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.")
        self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏', 'error': 'OpenAI API Key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω.', 'progress': 0, 'processed_count':0, 'total_to_process':0})
        return "Config error: OpenAI Key not configured."

    # progress_info –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è _async_logic –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–¥–∞—á–µ–π –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è FAILURE
    progress_info_ref: Dict[str, Any] = {
        "processed_count": 0,
        "total_to_process": 0,
        "current_step": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏",
        "progress": 0
    }

    async def _async_logic(task_instance, current_progress_info_ref: Dict[str, Any]):
        analyzed_count_in_batch = 0
        total_posts_for_batch = 0
        local_engine = None

        current_progress_info_ref.update({
            'current_step': '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∞–Ω–∞–ª–∏–∑—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–æ–≤',
            'progress': 5,
            'processed_count': 0,
            'total_to_process': 0
        })
        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)

            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏.")

            async with LocalAsyncSessionFactory_Task() as db_session:
                stmt = (
                    select(Post)
                    .where(or_(Post.text_content.isnot(None), Post.caption_text.isnot(None)))
                    .where(Post.post_sentiment_label.is_(None))
                    .order_by(Post.posted_at.asc())
                    .limit(limit_posts_to_process)
                )

                if channel_ids:
                    logger.info(f"{log_prefix} –ê–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤: {channel_ids}")
                    stmt = stmt.where(Post.channel_id.in_(channel_ids))
                else:
                    logger.info(f"{log_prefix} –ê–Ω–∞–ª–∏–∑ –¥–ª—è –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (–µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã).")
                    if not start_date_iso and not end_date_iso:
                         active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery()
                         stmt = stmt.join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id)

                if start_date_iso:
                    # –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è date.fromisoformat
                    dt_start_naive = date.fromisoformat(start_date_iso.split('T')[0])
                    dt_start = datetime(dt_start_naive.year, dt_start_naive.month, dt_start_naive.day, 0, 0, 0, 0, tzinfo=timezone.utc)
                    stmt = stmt.where(Post.posted_at >= dt_start)
                    logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä start_date: {dt_start}")
                if end_date_iso:
                    # –ò–ó–ú–ï–ù–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è date.fromisoformat
                    dt_end_naive = date.fromisoformat(end_date_iso.split('T')[0])
                    dt_end = datetime(dt_end_naive.year, dt_end_naive.month, dt_end_naive.day, 23, 59, 59, 999999, tzinfo=timezone.utc)
                    stmt = stmt.where(Post.posted_at <= dt_end)
                    logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä end_date: {dt_end}")

                posts_to_process_result = await db_session.execute(stmt)
                posts_to_process = posts_to_process_result.scalars().all()
                total_posts_for_batch = len(posts_to_process)
                current_progress_info_ref['total_to_process'] = total_posts_for_batch

                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ.")
                current_progress_info_ref.update({
                    'current_step': f'–ù–∞–π–¥–µ–Ω–æ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                    'progress': 10,
                })
                task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)


                if not posts_to_process:
                    logger.info(f"{log_prefix} –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞—á–∫–µ.")
                    result_message = "–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ."
                    current_progress_info_ref.update({
                        'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ: –Ω–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                        'progress': 100,
                        'result_summary': result_message
                    })
                    task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
                    return result_message

                for i, post_obj in enumerate(posts_to_process):
                    text_for_analysis = post_obj.caption_text if post_obj.caption_text and post_obj.caption_text.strip() else post_obj.text_content
                    if not text_for_analysis or not text_for_analysis.strip():
                        logger.info(f"{log_prefix}  –ü–æ—Å—Ç ID {post_obj.id} ({post_obj.link}) –Ω–µ –∏–º–µ–µ—Ç —Ç–µ–∫—Å—Ç–∞/–ø–æ–¥–ø–∏—Å–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π. –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ 'neutral' –±–µ–∑ –≤—ã–∑–æ–≤–∞ LLM.")
                        post_obj.post_sentiment_label = "neutral"
                        post_obj.post_sentiment_score = 0.0
                        post_obj.updated_at = datetime.now(timezone.utc)
                        db_session.add(post_obj)
                        analyzed_count_in_batch += 1 # –°—á–∏—Ç–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π, —Ö–æ—Ç—è LLM –Ω–µ –≤—ã–∑—ã–≤–∞–ª—Å—è
                        current_progress_info_ref['processed_count'] = analyzed_count_in_batch
                        # LLM –Ω–µ –≤—ã–∑—ã–≤–∞–ª—Å—è, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ª–æ–≥–∏–∫–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                    else:
                        logger.info(f"{log_prefix}  –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id} ({post_obj.link})...")
                        s_label, s_score = "neutral", 0.0 # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        try:
                            prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (JSON: sentiment_label: [positive,negative,neutral,mixed], sentiment_score: [-1.0,1.0]):\n---\n{text_for_analysis[:settings.LLM_MAX_PROMPT_LENGTH]}\n---\nJSON_RESPONSE:"
                            llm_response_str = await –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm(prompt, –º–æ–¥–µ–ª—å=settings.OPENAI_DEFAULT_MODEL_FOR_TASKS or "gpt-3.5-turbo-1106", —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞=0.2, –º–∞–∫—Å_—Ç–æ–∫–µ–Ω—ã=60, is_json_response_expected=True)
                            if llm_response_str:
                                try:
                                    data = json.loads(llm_response_str)
                                    s_label_candidate = data.get("sentiment_label")
                                    s_score_candidate_raw = data.get("sentiment_score")
                                    if s_label_candidate in ["positive", "negative", "neutral", "mixed"]: s_label = s_label_candidate
                                    else: logger.warning(f"{log_prefix}    LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π sentiment_label '{s_label_candidate}' –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω 'neutral'. –û—Ç–≤–µ—Ç: {llm_response_str}"); s_label = "neutral"
                                    if isinstance(s_score_candidate_raw, (int, float)) and -1.0 <= float(s_score_candidate_raw) <= 1.0: s_score = float(s_score_candidate_raw)
                                    else: logger.warning(f"{log_prefix}    LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π sentiment_score '{s_score_candidate_raw}' –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω 0.0. –û—Ç–≤–µ—Ç: {llm_response_str}"); s_score = 0.0
                                    if s_label_candidate is None and s_score_candidate_raw is None : s_label = "neutral" # –û–±–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, —Å—á–∏—Ç–∞–µ–º neutral
                                except (json.JSONDecodeError, TypeError, ValueError) as e_json: logger.error(f"{log_prefix}    –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç LLM –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id} ({type(e_json).__name__}: {e_json}). –û—Ç–≤–µ—Ç LLM: '{llm_response_str}'")
                            else: logger.warning(f"{log_prefix}    LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 'neutral'.")
                        except OpenAIError as e_llm_sentiment:
                            logger.error(f"{log_prefix}    !!! –û—à–∏–±–∫–∞ OpenAI API –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {type(e_llm_sentiment).__name__} - {e_llm_sentiment}");
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –ø–æ—Å—Ç, –æ–Ω –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ç–æ–π –ø–æ–ø—ã—Ç–∫–∏
                            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
                            continue
                        except Exception as e_sa_general:
                            logger.error(f"{log_prefix}    !!! –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {type(e_sa_general).__name__} - {e_sa_general}", exc_info=True);
                            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç –ø–æ—Å—Ç

                        post_obj.post_sentiment_label = s_label
                        post_obj.post_sentiment_score = s_score
                        post_obj.updated_at = datetime.now(timezone.utc)
                        db_session.add(post_obj)
                        analyzed_count_in_batch += 1
                        current_progress_info_ref['processed_count'] = analyzed_count_in_batch
                        logger.info(f"{log_prefix}    –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–∞ ID {post_obj.id}: {s_label} ({s_score:.2f}) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
                    # i - –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–≥–æ –ø–æ—Å—Ç–∞, analyzed_count_in_batch - —Å–∫–æ–ª—å–∫–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ (–∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–æ neutral)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º i+1 –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–∫–∞–∂–¥—ã–π 10-–π *–ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–π* –ø–æ—Å—Ç"
                    if (i + 1) % 10 == 0 or (i + 1) == total_posts_for_batch:
                        # –ü—Ä–æ–≥—Ä–µ—Å—Å —Å—á–∏—Ç–∞–µ–º –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø–æ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ *–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å* –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                        current_progress_percentage = 10 + int(((i + 1) / total_posts_for_batch) * 85) if total_posts_for_batch > 0 else 95
                        current_progress_info_ref.update({
                            'current_step': f'–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {analyzed_count_in_batch}/{total_posts_for_batch} (–ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ {i+1})',
                            'progress': current_progress_percentage
                        })
                        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

                # –ö–æ–º–º–∏—Ç, –µ—Å–ª–∏ –±—ã–ª–∏ –∫–∞–∫–∏–µ-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ neutral –ø—É—Å—Ç—ã–µ –ø–æ—Å—Ç—ã)
                if analyzed_count_in_batch > 0 or any(
                    (p.post_sentiment_label == "neutral" and not p.text_content and not p.caption_text) for p in posts_to_process
                ):
                    await db_session.commit()
                    logger.info(f"{log_prefix}  –û–±–Ω–æ–≤–ª–µ–Ω–æ {current_progress_info_ref['processed_count']} –ø–æ—Å—Ç–æ–≤ –≤ —ç—Ç–æ–π –ø–∞—á–∫–µ (–≤–∫–ª—é—á–∞—è –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ neutral).")


            result_message = f"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–∞—á–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ (—Å LLM –∏–ª–∏ –ø–æ–º–µ—á–µ–Ω–æ neutral): {current_progress_info_ref['processed_count']} –∏–∑ {total_posts_for_batch} –ø–æ—Å—Ç–æ–≤."
            current_progress_info_ref.update({
                'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ',
                'progress': 100,
                'result_summary': result_message
            })
            task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
            return result_message
        except Exception as e_async_main:
            logger.error(f"{log_prefix} !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_logic: {type(e_async_main).__name__} - {e_async_main}", exc_info=True)
            current_progress_info_ref.update({
                'current_step': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ async_logic: {type(e_async_main).__name__}',
                'error': str(e_async_main)
            })
            raise
        finally:
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally.")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_logic(self, progress_info_ref))
        task_duration = time.time() - task_start_time
        logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ SUCCESS —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤–Ω—É—Ç—Ä–∏ _async_logic
        return result_message
    except Exception as e_task_main:
        task_duration = time.time() - task_start_time
        logger.error(f"{log_prefix} !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_main).__name__} - {e_task_main}", exc_info=True)

        failure_meta = {
            'current_step': progress_info_ref.get('current_step', '–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏'),
            'error': str(e_task_main),
            'progress': progress_info_ref.get('progress', 0),
            'processed_count': progress_info_ref.get('processed_count', 0),
            'total_to_process': progress_info_ref.get('total_to_process', 0)
        }
        failure_meta['progress'] = 100 # –ó–∞–¥–∞—á–∞ –¥–æ—à–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ (—Å –æ—à–∏–±–∫–æ–π)
        failure_meta['current_step'] = f"–û—à–∏–±–∫–∞: {failure_meta['current_step']}"


        self.update_state(state='FAILURE', meta=failure_meta)

        try:
            if self.request.retries < self.max_retries:
                logger.info(f"{log_prefix} –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–¥–∞—á–∏ {self.request.id} ({self.request.retries + 1}/{self.max_retries}). –ó–∞–¥–µ—Ä–∂–∫–∞: {int(self.default_retry_delay * (2 ** self.request.retries))} —Å–µ–∫.")
                raise self.retry(exc=e_task_main, countdown=int(self.default_retry_delay * (2 ** self.request.retries)))
            else:
                logger.error(f"{log_prefix} Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}.")
                # –ò–ó–ú–ï–ù–ï–ù–û: —É–±—Ä–∞–Ω return. Celery —Å–∞–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞—Ç—É—Å FAILURE.
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —á–µ—Ä–µ–∑ self.update_state.
                # –ï—Å–ª–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç return, —Å—Ç–∞—Ç—É—Å –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å SUCCESS.
                # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ e_task_main "–≤—ã–ø–∞–¥–µ—Ç" –∏–∑ –∑–∞–¥–∞—á–∏, –∏ Celery –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –µ–≥–æ.
        except Exception as e_retry_logic:
            logger.error(f"{log_prefix} Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}: {type(e_retry_logic).__name__}", exc_info=True)
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –µ—Å–ª–∏ —Å–∞–º–∞ –ª–æ–≥–∏–∫–∞ retry –¥–∞–ª–∞ —Å–±–æ–π
            self.update_state(state='FAILURE', meta={**failure_meta, 'error': f"Retry logic failed: {str(e_retry_logic)}. Original error: {str(e_task_main)}"})
            # –ü–æ–∑–≤–æ–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∏–ª–∏ –Ω–æ–≤–æ–º—É –∏—Å–∫–ª—é—á–µ–Ω–∏—é "–≤—ã–ø–∞—Å—Ç—å"
            raise # –ü–µ—Ä–µ–≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã Celery –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª –∑–∞–¥–∞—á—É —Å FAILURE

@celery_instance.task(name="tasks.analyze_single_comment_ai_features", bind=True, max_retries=2, default_retry_delay=60 * 2)
def analyze_single_comment_ai_features_task(self, comment_id: int):
    task_start_time = time.time()
    log_prefix = "[AICommentFeaturesMock]" # –ò–∑–º–µ–Ω–∏–ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω–∞ '–∑–∞–≥–ª—É—à–∫–∞' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ comment_id: {comment_id} (Task ID: {self.request.id}). LLM –í–´–ó–û–í –û–¢–ö–õ–Æ–ß–ï–ù.")
    
    local_engine = None 
    
    async def _async_analyze_comment_logic():
        nonlocal local_engine
        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL 
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)
            
            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )

            async with LocalAsyncSessionFactory_Task() as db_session:
                # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –ø–æ ID
                comment_exists_id = await db_session.scalar(
                    select(Comment.id).where(Comment.id == comment_id)
                )

                if not comment_exists_id: # –ï—Å–ª–∏ scalar –≤–µ—Ä–Ω—É–ª None (–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω)
                    logger.warning(f"{log_prefix} –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å ID {comment_id} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º.")
                    return f"Comment ID {comment_id} not found for mock processing."

                # –ü—Ä–æ—Å—Ç–æ –ø–æ–º–µ—á–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–∫ "–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π" (–±–µ–∑ –≤—ã–∑–æ–≤–∞ LLM)
                # –ü–æ–ª—è extracted_topics –∏ —Ç.–¥. —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –≤ –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏
                update_values = {
                    "ai_analysis_completed_at": datetime.now(timezone.utc),
                    "extracted_topics": [], 
                    "extracted_problems": [],
                    "extracted_questions": [],
                    "extracted_suggestions": []
                }
                update_stmt = update(Comment).where(Comment.id == comment_id).values(**update_values)
                await db_session.execute(update_stmt)
                await db_session.commit()
                logger.info(f"{log_prefix} –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ID {comment_id} –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π (LLM –≤—ã–∑–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω).")
                return f"Comment ID {comment_id} mock-processed (LLM call skipped)."

        except Exception as e_general_comment_analysis:
            logger.error(f"{log_prefix} –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ mock-–æ–±—Ä–∞–±–æ—Ç–∫–µ comment_id {comment_id}: {type(e_general_comment_analysis).__name__} - {e_general_comment_analysis}", exc_info=True)
            raise 
        finally:
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally (comment_id: {comment_id}).")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_analyze_comment_logic())
        task_duration = time.time() - task_start_time
        logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' (–∑–∞–≥–ª—É—à–∫–∞) –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        return result_message
    except Exception as e_task_level:
        task_duration = time.time() - task_start_time; logger.error(f"{log_prefix} !!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞–≥–ª—É—à–∫–∞) (comment_id: {comment_id}) (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}", exc_info=True)
        try:
            if self.request.retries < self.max_retries:
                default_retry_delay_val = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 120
                countdown = int(default_retry_delay_val * (2 ** self.request.retries))
                logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}) —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫")
                raise self.retry(exc=e_task_level, countdown=countdown)
            else:
                logger.error(f"Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}).")
                raise e_task_level
        except Exception as e_retry_logic:
            logger.error(f"Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}): {type(e_retry_logic).__name__}", exc_info=True)
            raise e_task_level

@celery_instance.task(name="tasks.enqueue_comments_for_ai_feature_analysis", bind=True, max_retries=2, default_retry_delay=180) # –£–º–µ–Ω—å—à–∏–ª default_retry_delay
def enqueue_comments_for_ai_feature_analysis_task(
    self,
    limit_comments_to_queue: Optional[int] = None,
    older_than_hours: Optional[int] = None,
    channel_id_filter: Optional[int] = None,
    process_only_recent_hours: Optional[int] = None,
    comment_ids_to_process: Optional[List[int]] = None,
    start_date_iso: Optional[str] = None, # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤, –∫ –∫–æ—Ç–æ—Ä—ã–º –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
    end_date_iso: Optional[str] = None    # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤, –∫ –∫–æ—Ç–æ—Ä—ã–º –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
):
    actual_limit_comments_to_queue = limit_comments_to_queue if limit_comments_to_queue is not None else settings.COMMENT_ENQUEUE_BATCH_SIZE
    task_start_time = time.time()
    log_prefix_parts = ["[AICommentQueue"]
    if channel_id_filter: log_prefix_parts.append(f"Ch:{channel_id_filter}")
    if start_date_iso: log_prefix_parts.append(f"StartPost:{start_date_iso}") # –£—Ç–æ—á–Ω–∏–ª, —á—Ç–æ —ç—Ç–æ –¥–ª—è –ø–æ—Å—Ç–æ–≤
    if end_date_iso: log_prefix_parts.append(f"EndPost:{end_date_iso}")     # –£—Ç–æ—á–Ω–∏–ª, —á—Ç–æ —ç—Ç–æ –¥–ª—è –ø–æ—Å—Ç–æ–≤
    if comment_ids_to_process: log_prefix_parts.append(f"IDs:{len(comment_ids_to_process)}")
    if older_than_hours is not None: log_prefix_parts.append(f"OlderThanH:{older_than_hours}")
    if process_only_recent_hours is not None: log_prefix_parts.append(f"RecentH:{process_only_recent_hours}")
    log_prefix = "".join(log_prefix_parts) + "]"

    logger.info(f"{log_prefix} –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏. –õ–∏–º–∏—Ç –ø–∞—á–∫–∏ (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω—ã ID): {actual_limit_comments_to_queue} (Task ID: {self.request.id})")

    progress_info_ref: Dict[str, Any] = {
        "processed_count": 0, # –°–∫–æ–ª—å–∫–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –±—ã–ª–æ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å
        "total_to_process": 0, # –°–∫–æ–ª—å–∫–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
        "current_step": "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –æ—á–µ—Ä–µ–¥—å",
        "progress": 0
    }

    async def _async_enqueue_logic(task_instance, current_progress_info_ref: Dict[str, Any]):
        enqueued_count_local = 0 # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è _async_logic
        total_found_for_queue = 0
        local_engine = None

        current_progress_info_ref.update({
            'current_step': '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø–æ–∏—Å–∫—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏',
            'progress': 5
        })
        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

        try:
            ASYNC_DB_URL_TASK = settings.DATABASE_URL
            if not ASYNC_DB_URL_TASK.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL_TASK = ASYNC_DB_URL_TASK.replace("postgresql://", "postgresql+asyncpg://", 1)

            local_engine = create_async_engine(ASYNC_DB_URL_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory_Task = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏.")

            async with LocalAsyncSessionFactory_Task() as db_session:
                comment_ids_to_enqueue: List[int] = []
                if comment_ids_to_process is not None:
                    logger.info(f"{log_prefix} –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö ID –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comment_ids_to_process)} —à—Ç.")
                    if comment_ids_to_process:
                        stmt_filter_ids = select(Comment.id)\
                            .where(Comment.id.in_(comment_ids_to_process))\
                            .where(Comment.text_content.isnot(None))\
                            .where(Comment.text_content != "")\
                            .where(Comment.ai_analysis_completed_at.is_(None)) # –¢–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—â–µ –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å

                        filtered_ids_result = await db_session.execute(stmt_filter_ids)
                        comment_ids_to_enqueue = filtered_ids_result.scalars().all()
                        logger.info(f"{log_prefix} –ò–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö {len(comment_ids_to_process)} ID, {len(comment_ids_to_enqueue)} –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ –∏ –±—É–¥—É—Ç –ø–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –æ—á–µ—Ä–µ–¥—å.")
                    else:
                        comment_ids_to_enqueue = []
                else:
                    # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª–∏
                    stmt = select(Comment.id).where(Comment.text_content.isnot(None)).where(Comment.text_content != "")

                    # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º Post –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –∫–∞–Ω–∞–ª—É –∏–ª–∏ –¥–∞—Ç–∞–º –ø–æ—Å—Ç–æ–≤
                    # –≠—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
                    needs_join_post = bool(channel_id_filter or start_date_iso or end_date_iso)
                    if needs_join_post:
                        stmt = stmt.join(Post, Comment.post_id == Post.id)
                        if channel_id_filter:
                            stmt = stmt.where(Post.channel_id == channel_id_filter)

                        if start_date_iso:
                            dt_start_naive = date.fromisoformat(start_date_iso.split('T')[0])
                            dt_start = datetime(dt_start_naive.year, dt_start_naive.month, dt_start_naive.day, 0, 0, 0, 0, tzinfo=timezone.utc)
                            stmt = stmt.where(Post.posted_at >= dt_start)
                            logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä start_date –¥–ª—è –ø–æ—Å—Ç–æ–≤: {dt_start}")
                        if end_date_iso:
                            dt_end_naive = date.fromisoformat(end_date_iso.split('T')[0])
                            dt_end = datetime(dt_end_naive.year, dt_end_naive.month, dt_end_naive.day, 23, 59, 59, 999999, tzinfo=timezone.utc)
                            stmt = stmt.where(Post.posted_at <= dt_end)
                            logger.info(f"{log_prefix} –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä end_date –¥–ª—è –ø–æ—Å—Ç–æ–≤: {dt_end}")

                    # –§–∏–ª—å—Ç—Ä—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ç–∞—Ç—É—Å—É –∞–Ω–∞–ª–∏–∑–∞
                    if process_only_recent_hours is not None and process_only_recent_hours > 0:
                        time_threshold_recent = datetime.now(timezone.utc) - timedelta(hours=process_only_recent_hours)
                        stmt = stmt.where(Comment.commented_at >= time_threshold_recent)
                        stmt = stmt.where(Comment.ai_analysis_completed_at.is_(None)) # –¢–æ–ª—å–∫–æ –Ω–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
                        stmt = stmt.order_by(Comment.commented_at.desc()) # –°–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ
                        logger.info(f"{log_prefix} –†–µ–∂–∏–º –Ω–µ–¥–∞–≤–Ω–∏—Ö: –∑–∞ {process_only_recent_hours}—á, —Ç–æ–ª—å–∫–æ –Ω–µ–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ.")
                    else:
                        if older_than_hours is not None:
                            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ, —á—Ç–æ –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –ò–õ–ò —Ç–µ, —á–µ–π –∞–Ω–∞–ª–∏–∑ —É—Å—Ç–∞—Ä–µ–ª
                            time_threshold_older = datetime.now(timezone.utc) - timedelta(hours=older_than_hours)
                            stmt = stmt.where(
                                or_(
                                    Comment.ai_analysis_completed_at.is_(None),
                                    Comment.ai_analysis_completed_at < time_threshold_older
                                )
                            )
                            logger.info(f"{log_prefix} –†–µ–∂–∏–º –±—ç–∫–ª–æ–≥–∞ —Å –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–æ–º —Å—Ç–∞—Ä—ã—Ö (—Å—Ç–∞—Ä—à–µ {older_than_hours}—á).")
                        else:
                            # –¢–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—â–µ –Ω–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å
                            stmt = stmt.where(Comment.ai_analysis_completed_at.is_(None))
                            logger.info(f"{log_prefix} –†–µ–∂–∏–º –±—ç–∫–ª–æ–≥–∞ (—Ç–æ–ª—å–∫–æ –Ω–µ–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ).")
                        stmt = stmt.order_by(Comment.commented_at.asc()) # –°–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∏–∑ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö

                    stmt = stmt.limit(actual_limit_comments_to_queue)
                    comment_ids_result = await db_session.execute(stmt)
                    comment_ids_to_enqueue = comment_ids_result.scalars().all()

                total_found_for_queue = len(comment_ids_to_enqueue)
                current_progress_info_ref['total_to_process'] = total_found_for_queue

                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {total_found_for_queue} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏.")
                current_progress_info_ref.update({
                    'current_step': f'–ù–∞–π–¥–µ–Ω–æ {total_found_for_queue} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å',
                    'progress': 20 # –£–≤–µ–ª–∏—á–∏–ª–∏ –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
                })
                task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)


                if not comment_ids_to_enqueue:
                    logger.info(f"{log_prefix} –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.")
                    result_message = "No comments found to queue."
                    current_progress_info_ref.update({
                        'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ: –Ω–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –æ—á–µ—Ä–µ–¥–∏',
                        'progress': 100,
                        'result_summary': result_message
                    })
                    task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
                    return result_message

                logger.info(f"{log_prefix} –ù–∞—á–∏–Ω–∞—é –ø–æ—Å—Ç–∞–Ω–æ–≤–∫—É –≤ –æ—á–µ—Ä–µ–¥—å {total_found_for_queue} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤...")
                for i, comment_id_to_process in enumerate(comment_ids_to_enqueue):
                    analyze_single_comment_ai_features_task.delay(comment_id_to_process)
                    enqueued_count_local += 1
                    current_progress_info_ref['processed_count'] = enqueued_count_local

                    if (i + 1) % 20 == 0 or (i + 1) == total_found_for_queue: # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 20 –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ
                        current_progress_percentage = 20 + int(((i + 1) / total_found_for_queue) * 75) if total_found_for_queue > 0 else 95
                        current_progress_info_ref.update({
                            'current_step': f'–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ –æ—á–µ—Ä–µ–¥—å: {enqueued_count_local}/{total_found_for_queue}',
                            'progress': current_progress_percentage
                        })
                        task_instance.update_state(state='PROGRESS', meta=current_progress_info_ref)

                result_message = f"Successfully enqueued {enqueued_count_local} comments for detailed AI analysis."
                current_progress_info_ref.update({
                    'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ',
                    'progress': 100,
                    'result_summary': result_message
                })
                task_instance.update_state(state='SUCCESS', meta=current_progress_info_ref)
                return result_message

        except Exception as e_enqueue:
            logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –≤ _async_enqueue_logic: {type(e_enqueue).__name__} - {e_enqueue}", exc_info=True)
            current_progress_info_ref.update({
                'current_step': f'–û—à–∏–±–∫–∞ –≤ async_logic: {type(e_enqueue).__name__}',
                'error': str(e_enqueue)
            })
            raise # –ü–µ—Ä–µ–≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–∏ –∑–∞–¥–∞—á–∏
        finally:
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally.")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_enqueue_logic(self, progress_info_ref))
        task_duration = time.time() - task_start_time
        logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ SUCCESS —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤–Ω—É—Ç—Ä–∏ _async_logic
        return result_message
    except Exception as e_task_level:
        task_duration = time.time() - task_start_time
        logger.error(f"{log_prefix} !!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}", exc_info=True)
        
        failure_meta = {
            'current_step': progress_info_ref.get('current_step', '–û—à–∏–±–∫–∞ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –æ—á–µ—Ä–µ–¥—å'),
            'error': str(e_task_level),
            'progress': progress_info_ref.get('progress', 0),
            'processed_count': progress_info_ref.get('processed_count', 0),
            'total_to_process': progress_info_ref.get('total_to_process', 0)
        }
        failure_meta['progress'] = 100 # –ó–∞–¥–∞—á–∞ –¥–æ—à–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ (—Å –æ—à–∏–±–∫–æ–π)
        failure_meta['current_step'] = f"–û—à–∏–±–∫–∞: {failure_meta['current_step']}"

        self.update_state(state='FAILURE', meta=failure_meta)

        try:
            if self.request.retries < self.max_retries:
                logger.info(f"{log_prefix} –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä–∞ –∑–∞–¥–∞—á–∏ {self.request.id} ({self.request.retries + 1}/{self.max_retries}). –ó–∞–¥–µ—Ä–∂–∫–∞: {int(self.default_retry_delay * (2 ** self.request.retries))} —Å–µ–∫.")
                raise self.retry(exc=e_task_level, countdown=int(self.default_retry_delay * (2 ** self.request.retries)))
            else:
                logger.error(f"{log_prefix} Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}.")
                # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ e_task_level "–≤—ã–ø–∞–¥–µ—Ç", Celery –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –µ–≥–æ.
        except Exception as e_retry_logic: # –õ–æ–≤–∏–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏–∑ self.retry
            logger.error(f"{log_prefix} Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}: {type(e_retry_logic).__name__}", exc_info=True)
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ FAILURE —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –µ—Å–ª–∏ —Å–∞–º–∞ –ª–æ–≥–∏–∫–∞ retry –ø–∞–¥–∞–µ—Ç
            self.update_state(state='FAILURE', meta={**failure_meta, 'error': f"Retry logic failed: {str(e_retry_logic)}. Original error: {str(e_task_level)}"})
            raise # –ü–µ—Ä–µ–≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º, —á—Ç–æ–±—ã Celery –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª –∑–∞–¥–∞—á—É —Å FAILURE

@celery_instance.task(name="tasks.advanced_data_refresh", bind=True, max_retries=2, default_retry_delay=60 * 10)
def advanced_data_refresh_task(
    self,
    channel_ids: Optional[List[int]] = None,
    post_refresh_mode_str: str = PostRefreshMode.NEW_ONLY.value,
    post_refresh_days: Optional[int] = None, 
    post_refresh_start_date_iso: Optional[str] = None,
    post_limit_per_channel: int = 100,
    update_existing_posts_info: bool = False,
    comment_refresh_mode_str: str = CommentRefreshMode.ADD_NEW_TO_EXISTING.value,
    comment_limit_per_post: int = settings.COMMENT_FETCH_LIMIT,
    analyze_new_comments: bool = True
):
    task_start_time = time.time()
    log_prefix = "[AdvancedRefresh]"
    
    logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω–∞ –∑–∞–¥–∞—á–∞ (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE). ID: {self.request.id}. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: channels={channel_ids}, post_mode='{post_refresh_mode_str}', post_days={post_refresh_days}, post_start_date='{post_refresh_start_date_iso}', post_limit={post_limit_per_channel}, update_existing={update_existing_posts_info}, comment_mode='{comment_refresh_mode_str}', comment_limit={comment_limit_per_post}, analyze={analyze_new_comments}")
    
    self.update_state(state='PROGRESS', meta={'current_step': '–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ (–õ–æ–∫–∞–ª—å–Ω—ã–π Engine)', 'progress': 5})
    
    post_refresh_mode_enum: PostRefreshMode
    comment_refresh_mode_enum: CommentRefreshMode
    post_refresh_start_date_dt: Optional[datetime] = None # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
    
    try:
        post_refresh_mode_enum = PostRefreshMode(post_refresh_mode_str)
        comment_refresh_mode_enum = CommentRefreshMode(comment_refresh_mode_str)
        
        if post_refresh_start_date_iso:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –¥–∞—Ç—É, –≤—Ä–µ–º—è –±—É–¥–µ—Ç 00:00:00 UTC
            parsed_date_obj = date.fromisoformat(post_refresh_start_date_iso.split('T')[0])
            post_refresh_start_date_dt = datetime(parsed_date_obj.year, parsed_date_obj.month, parsed_date_obj.day, tzinfo=timezone.utc)
            logger.info(f"{log_prefix} post_refresh_start_date_dt —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {post_refresh_start_date_dt.isoformat()}")

    except ValueError as e:
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–¥–∞—á–∏', 'progress': 100, 'error': str(e)})
        return f"–û—à–∏–±–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–¥–∞—á–∏: {e}"
    
    api_id_val = settings.TELEGRAM_API_ID
    api_hash_val = settings.TELEGRAM_API_HASH
    session_file_path = "/app/celery_telegram_session"

    if not all([api_id_val, api_hash_val]): 
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: Telegram API ID/Hash –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã.")
        self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Telegram API', 'progress': 100, 'error': 'Credentials (ID/Hash) not configured'})
        return "Config error: Telegram API ID/Hash"

    async def _async_advanced_refresh_logic():
        tg_client = None
        local_engine = None 
        processed_channels_count = 0
        total_new_posts_task = 0
        total_updated_posts_info_task = 0 # –û–±—â–∏–π —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å—Ç–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –æ–±–Ω–æ–≤–∏–ª–∞—Å—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        total_new_comments_collected_task = 0
        newly_added_comment_ids_for_ai_task: List[int] = [] # –¢–æ–ª—å–∫–æ ID –Ω–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–æ–≤ –¥–ª—è AI
        
        try:
            ASYNC_DB_URL = settings.DATABASE_URL 
            if not ASYNC_DB_URL.startswith("postgresql+asyncpg://"):
                ASYNC_DB_URL = ASYNC_DB_URL.replace("postgresql://", "postgresql+asyncpg://", 1)
            
            local_engine = create_async_engine(ASYNC_DB_URL, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(
                bind=local_engine, class_=AsyncSession, expire_on_commit=False, autoflush=False, autocommit=False
            )
            logger.info(f"{log_prefix} –õ–æ–∫–∞–ª—å–Ω—ã–π async_engine –∏ AsyncSessionFactory —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è –∑–∞–¥–∞—á–∏.")

            async with LocalAsyncSessionFactory() as db: 
                logger.info(f"{log_prefix} –°–æ–∑–¥–∞–Ω–∏–µ TGClient: {session_file_path}")
                tg_client = TelegramClient(session_file_path, api_id_val, api_hash_val)
                self.update_state(state='PROGRESS', meta={'current_step': '–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Telegram', 'progress': 10})
                await tg_client.connect()
                if not await tg_client.is_user_authorized():
                    self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ Telegram', 'error': 'TG Client not authorized'})
                    raise ConnectionRefusedError(f"Celery: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω –¥–ª—è {session_file_path}.session")
                me = await tg_client.get_me()
                logger.info(f"{log_prefix} TGClient –ø–æ–¥–∫–ª—é—á–µ–Ω –∫–∞–∫: {me.first_name if me else 'N/A'}")
                
                channels_to_process_q = select(Channel).where(Channel.is_active == True)
                if channel_ids is not None:
                    if not any(channel_ids): 
                        logger.info(f"{log_prefix} –ü–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤.")
                        self.update_state(state='SUCCESS', meta={'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ (–ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤)', 'progress': 100, 'result_summary': '–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.'})
                        return "–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."
                    channels_to_process_q = channels_to_process_q.where(Channel.id.in_(channel_ids))
                
                channels_result = await db.execute(channels_to_process_q)
                channels_db_list: List[Channel] = channels_result.scalars().all()

                if not channels_db_list:
                    logger.info(f"{log_prefix} –ù–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                    self.update_state(state='SUCCESS', meta={'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ (–Ω–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)', 'progress': 100, 'result_summary': '–ù–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.'})
                    return "–ù–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."
                
                total_channels_to_process = len(channels_db_list)
                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {total_channels_to_process} –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                base_progress = 15

                for idx, channel_db_obj in enumerate(channels_db_list):
                    processed_channels_count += 1
                    channel_progress = base_progress + int(((idx + 1) / total_channels_to_process) * 70) 
                    self.update_state(state='PROGRESS', meta={'current_step': f'–ö–∞–Ω–∞–ª: {channel_db_obj.title} ({idx+1}/{total_channels_to_process})', 'progress': channel_progress, 'channel_id': channel_db_obj.id, 'channel_title': channel_db_obj.title})
                    logger.info(f"{log_prefix} –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: '{channel_db_obj.title}' (ID: {channel_db_obj.id})")
                    
                    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–Ω–∞–ª–∞
                    channel_new_posts = 0
                    channel_updated_posts_info = 0
                    channel_new_comments = 0
                    
                    try:
                        tg_channel_entity = await tg_client.get_entity(channel_db_obj.id)
                        
                        posts_to_scan_comments_for: List[Post] = [] 
                        
                        latest_post_id_tg_for_channel_update = channel_db_obj.last_processed_post_id or 0

                        if post_refresh_mode_enum == PostRefreshMode.UPDATE_STATS_ONLY:
                            logger.info(f"{log_prefix}  –†–µ–∂–∏–º UPDATE_STATS_ONLY –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.title}.")
                            
                            comment_count_subquery = (
                                select(
                                    Comment.post_id,
                                    func.count(Comment.id).label("db_comment_count")
                                )
                                .group_by(Comment.post_id)
                                .subquery("comment_counts")
                            )
                            db_posts_stmt = (
                                select(Post, func.coalesce(comment_count_subquery.c.db_comment_count, 0).label("db_comment_count_for_post"))
                                .outerjoin(comment_count_subquery, Post.id == comment_count_subquery.c.post_id)
                                .where(Post.channel_id == channel_db_obj.id)
                                .order_by(Post.telegram_post_id.desc()) # –°–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                            )
                            
                            if post_refresh_days:
                                date_limit = datetime.now(timezone.utc) - timedelta(days=post_refresh_days)
                                db_posts_stmt = db_posts_stmt.where(Post.posted_at >= date_limit)
                                logger.info(f"{log_prefix}    UPDATE_STATS_ONLY: –ø—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä –ø–æ post_refresh_days ({post_refresh_days} –¥–Ω–µ–π)")
                            elif post_refresh_start_date_dt:
                                db_posts_stmt = db_posts_stmt.where(Post.posted_at >= post_refresh_start_date_dt)
                                logger.info(f"{log_prefix}    UPDATE_STATS_ONLY: –ø—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä –ø–æ post_refresh_start_date ({post_refresh_start_date_dt.isoformat()})")
                            
                            if post_limit_per_channel and post_limit_per_channel > 0 :
                                db_posts_stmt = db_posts_stmt.limit(post_limit_per_channel)
                                logger.info(f"{log_prefix}    UPDATE_STATS_ONLY: –ø—Ä–∏–º–µ–Ω–µ–Ω –ª–∏–º–∏—Ç {post_limit_per_channel} –ø–æ—Å—Ç–æ–≤ –∏–∑ –ë–î –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")

                            db_posts_data_list_result = await db.execute(db_posts_stmt)
                            db_posts_data_list = db_posts_data_list_result.all() 

                            if db_posts_data_list:
                                telegram_post_ids_to_fetch = [p_data[0].telegram_post_id for p_data in db_posts_data_list]
                                logger.info(f"{log_prefix}    –ù–∞–π–¥–µ–Ω–æ {len(db_posts_data_list)} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.title} –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–∏–ª—å—Ç—Ä–∞–º).")
                                
                                batch_size = 100 
                                for i in range(0, len(telegram_post_ids_to_fetch), batch_size):
                                    batch_ids = telegram_post_ids_to_fetch[i:i + batch_size]
                                    fetched_tg_messages: Optional[List[Message]] = None
                                    logger.info(f"{log_prefix}    –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∏–∑ Telegram –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–∞—á–∫–∏ –∏–∑ {len(batch_ids)} –ø–æ—Å—Ç–æ–≤...")
                                    try:
                                        messages_or_none = await tg_client.get_messages(tg_channel_entity, ids=batch_ids)
                                        if messages_or_none:
                                            fetched_tg_messages = [msg for msg in messages_or_none if msg is not None and not isinstance(msg, MessageService)] 
                                    except Exception as e_get_msgs_batch:
                                        logger.error(f"{log_prefix}    –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–º –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ ID –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.id}: {e_get_msgs_batch}")
                                        continue 

                                    if fetched_tg_messages:
                                        logger.info(f"{log_prefix}      –ü–æ–ª—É—á–µ–Ω–æ {len(fetched_tg_messages)} –ù–ï —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")
                                        
                                        db_posts_map_with_counts = {
                                            p_data[0].telegram_post_id: (p_data[0], p_data[1]) 
                                            for p_data in db_posts_data_list 
                                            if p_data[0].telegram_post_id in batch_ids
                                        }

                                        for tg_message in fetched_tg_messages:
                                            if tg_message.id in db_posts_map_with_counts:
                                                post_in_db, db_comment_count = db_posts_map_with_counts[tg_message.id]
                                                
                                                api_comments_count_tg = tg_message.replies.replies if tg_message.replies and tg_message.replies.replies is not None else 0
                                                
                                                info_changed_for_post = False
                                                if update_existing_posts_info: 
                                                    if post_in_db.views_count != tg_message.views: info_changed_for_post = True; post_in_db.views_count = tg_message.views
                                                    new_reactions = await _process_reactions_for_db(tg_message.reactions)
                                                    if post_in_db.reactions != new_reactions : info_changed_for_post = True; post_in_db.reactions = new_reactions
                                                    if post_in_db.forwards_count != tg_message.forwards: info_changed_for_post = True; post_in_db.forwards_count = tg_message.forwards
                                                    new_edited_at = tg_message.edit_date.replace(tzinfo=timezone.utc) if tg_message.edit_date else None
                                                    if post_in_db.edited_at != new_edited_at: info_changed_for_post = True; post_in_db.edited_at = new_edited_at
                                                    
                                                    new_text, new_caption = (None, tg_message.text) if tg_message.media and tg_message.text else (tg_message.text if not tg_message.media else None, None)
                                                    if post_in_db.text_content != new_text: info_changed_for_post = True; post_in_db.text_content = new_text
                                                    if post_in_db.caption_text != new_caption: info_changed_for_post = True; post_in_db.caption_text = new_caption
                                                    
                                                    new_media_type, new_media_info = await _process_media_for_db(tg_message.media)
                                                    if post_in_db.media_type != new_media_type or post_in_db.media_content_info != new_media_info:
                                                        info_changed_for_post = True; post_in_db.media_type = new_media_type; post_in_db.media_content_info = new_media_info
                                                    
                                                    if (tg_message.pinned or False) != post_in_db.is_pinned: info_changed_for_post = True; post_in_db.is_pinned = tg_message.pinned or False
                                                    if tg_message.post_author != post_in_db.author_signature: info_changed_for_post = True; post_in_db.author_signature = tg_message.post_author
                                                    
                                                    if info_changed_for_post:
                                                        channel_updated_posts_info +=1
                                                        post_in_db.updated_at = datetime.now(timezone.utc)
                                                
                                                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –∫–æ–º–º–µ–Ω—Ç–æ–≤ –≤ Post –≤—Å–µ–≥–¥–∞, –µ—Å–ª–∏ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç TG
                                                if post_in_db.comments_count != api_comments_count_tg:
                                                    logger.debug(f"{log_prefix}      –ü–æ—Å—Ç TG ID {post_in_db.telegram_post_id}: comments_count –æ–±–Ω–æ–≤–ª–µ–Ω —Å {post_in_db.comments_count} –Ω–∞ {api_comments_count_tg} –∏–∑ Telegram API.")
                                                    post_in_db.comments_count = api_comments_count_tg
                                                    if not info_changed_for_post : # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Å—á–µ—Ç—á–∏–∫ –∫–æ–º–º–µ–Ω—Ç–æ–≤ –∏–∑–º–µ–Ω–∏–ª—Å—è
                                                        post_in_db.updated_at = datetime.now(timezone.utc)
                                                        # –ù–µ —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –∫–∞–∫ channel_updated_posts_info, –µ—Å–ª–∏ update_existing_posts_info=False
                                                        # –ù–æ –µ—Å–ª–∏ update_existing_posts_info=True, —Ç–æ —ç—Ç–æ —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–Ω–æ –≤—ã—à–µ
                                                
                                                db.add(post_in_db) 

                                                if api_comments_count_tg > db_comment_count:
                                                    logger.info(f"{log_prefix}      –ü–æ—Å—Ç TG ID {post_in_db.telegram_post_id}: —Å—á–µ—Ç—á–∏–∫ –∫–æ–º–º. —É–≤–µ–ª–∏—á–∏–ª—Å—è (API: {api_comments_count_tg}, DB –±—ã–ª–æ: {db_comment_count}). –°—Ç–∞–≤–∏–º –Ω–∞ —Å–±–æ—Ä –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö.")
                                                    posts_to_scan_comments_for.append(post_in_db)
                                                elif comment_refresh_mode_enum == CommentRefreshMode.ADD_NEW_TO_EXISTING and api_comments_count_tg > 0 :
                                                    # –ï—Å–ª–∏ —Ä–µ–∂–∏–º "–¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º", –∏ –≤ API –µ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç—ã (–¥–∞–∂–µ –µ—Å–ª–∏ —Å—á–µ—Ç—á–∏–∫ –Ω–µ –≤—ã—Ä–æ—Å),
                                                    # —Ç–æ —Å—Ç–æ–∏—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –ø–æ—è–≤–∏–ª–∏—Å—å –ª–∏ ID, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç —É –Ω–∞—Å.
                                                    # –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏, –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ —É–¥–∞–ª–∏–ª —Å—Ç–∞—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç –∏ –¥–æ–±–∞–≤–∏–ª –Ω–æ–≤—ã–π, –∏ –æ–±—â–µ–µ —á–∏—Å–ª–æ –æ—Å—Ç–∞–ª–æ—Å—å —Ç–µ–º –∂–µ.
                                                    logger.info(f"{log_prefix}      –ü–æ—Å—Ç TG ID {post_in_db.telegram_post_id}: —Å—á–µ—Ç—á–∏–∫ –∫–æ–º–º. –Ω–µ –≤—ã—Ä–æ—Å ({api_comments_count_tg}), –Ω–æ —Ä–µ–∂–∏–º ADD_NEW_TO_EXISTING. –°—Ç–∞–≤–∏–º –Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–æ–≤—ã—Ö ID.")
                                                    posts_to_scan_comments_for.append(post_in_db)
                                                else:
                                                    logger.debug(f"{log_prefix}      –ü–æ—Å—Ç TG ID {post_in_db.telegram_post_id}: —Å—á–µ—Ç—á–∏–∫ –∫–æ–º–º. –Ω–µ —É–≤–µ–ª–∏—á–∏–ª—Å—è (API: {api_comments_count_tg}, DB –±—ã–ª–æ: {db_comment_count}). –°–±–æ—Ä –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∫–æ–º–º. –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                            else:
                                logger.info(f"{log_prefix}    –ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –≤ –ë–î –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.title} –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–∏–ª—å—Ç—Ä–∞–º).")

                        else: # –†–µ–∂–∏–º—ã NEW_ONLY, LAST_N_DAYS, SINCE_DATE
                            iter_params_helper = {"entity": tg_channel_entity, "limit": post_limit_per_channel} # –õ–∏–º–∏—Ç –¥–ª—è helper'–∞
                            if post_refresh_mode_enum == PostRefreshMode.NEW_ONLY:
                                if channel_db_obj.last_processed_post_id:
                                    iter_params_helper["min_id"] = channel_db_obj.last_processed_post_id
                                else: # –ï—Å–ª–∏ last_processed_post_id –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å INITIAL_POST_FETCH_START_DATETIME, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                                    if settings.INITIAL_POST_FETCH_START_DATETIME:
                                       iter_params_helper["offset_date"] = settings.INITIAL_POST_FETCH_START_DATETIME
                                       iter_params_helper["reverse"] = True
                                    # –ï—Å–ª–∏ –∏ –µ–≥–æ –Ω–µ—Ç, helper –±—É–¥–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ post_limit_per_channel –ø–æ—Å—Ç–æ–≤
                            elif post_refresh_mode_enum == PostRefreshMode.LAST_N_DAYS and post_refresh_days:
                                iter_params_helper["offset_date"] = datetime.now(timezone.utc) - timedelta(days=post_refresh_days)
                                iter_params_helper["reverse"] = True
                            elif post_refresh_mode_enum == PostRefreshMode.SINCE_DATE and post_refresh_start_date_dt:
                                iter_params_helper["offset_date"] = post_refresh_start_date_dt
                                iter_params_helper["reverse"] = True
                            
                            all_processed_posts_from_helper, newly_created_posts_from_helper, num_new_p, num_upd_p, last_id_tg_helper = await _helper_fetch_and_process_posts_for_channel(
                                tg_client, db, channel_db_obj, iter_params_helper, 
                                update_existing_posts_info, log_prefix
                            )
                            channel_new_posts += num_new_p
                            channel_updated_posts_info += num_upd_p 
                            latest_post_id_tg_for_channel_update = max(latest_post_id_tg_for_channel_update, last_id_tg_helper)

                            if comment_refresh_mode_enum == CommentRefreshMode.NEW_POSTS_ONLY:
                                posts_to_scan_comments_for.extend(newly_created_posts_from_helper)
                            else: # ADD_NEW_TO_EXISTING
                                posts_to_scan_comments_for.extend(all_processed_posts_from_helper)
                        
                        if post_refresh_mode_enum == PostRefreshMode.NEW_ONLY and latest_post_id_tg_for_channel_update > (channel_db_obj.last_processed_post_id or 0):
                            channel_db_obj.last_processed_post_id = latest_post_id_tg_for_channel_update
                            db.add(channel_db_obj)

                        if comment_refresh_mode_enum != CommentRefreshMode.DO_NOT_REFRESH:
                            unique_posts_for_comment_scan = []
                            seen_post_ids_for_scan = set()
                            for post_obj in posts_to_scan_comments_for: # posts_to_scan_comments_for —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ
                                if post_obj.id not in seen_post_ids_for_scan:
                                    unique_posts_for_comment_scan.append(post_obj)
                                    seen_post_ids_for_scan.add(post_obj.id)

                            if unique_posts_for_comment_scan: 
                                logger.info(f"{log_prefix}  –°–±–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è {len(unique_posts_for_comment_scan)} –ø–æ—Å—Ç–æ–≤ –∫–∞–Ω–∞–ª–∞ {channel_db_obj.id} (—Ä–µ–∂–∏–º –∫–æ–º–º: {comment_refresh_mode_enum.value})...")
                                for post_obj_to_scan in unique_posts_for_comment_scan:
                                    num_c, new_c_ids = await _helper_fetch_and_process_comments_for_post(
                                        tg_client, db, post_obj_to_scan, tg_channel_entity, 
                                        comment_limit_per_post, log_prefix
                                    )
                                    channel_new_comments += num_c
                                    newly_added_comment_ids_for_ai_task.extend(new_c_ids)
                            else:
                                logger.info(f"{log_prefix}  –ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –∫–∞–Ω–∞–ª–µ {channel_db_obj.id} (—Å–æ–≥–ª–∞—Å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ).")
                        else:
                            logger.info(f"{log_prefix}  –°–±–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø—Ä–æ–ø—É—â–µ–Ω –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.id} (—Ä–µ–∂–∏–º –∫–æ–º–º: DO_NOT_REFRESH).")
                            
                    except (ChannelPrivateError, UsernameInvalidError, UsernameNotOccupiedError) as e_ch_access:
                        logger.warning(f"{log_prefix}  –ö–∞–Ω–∞–ª {channel_db_obj.id} ('{channel_db_obj.title}') –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e_ch_access}. –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º.")
                        channel_db_obj.is_active = False; db.add(channel_db_obj)
                        continue 
                    except FloodWaitError as fwe_ch:
                        logger.warning(f"{log_prefix}  FloodWait ({fwe_ch.seconds} —Å–µ–∫.) –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.title}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–Ω–∞–ª –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ.")
                        await asyncio.sleep(fwe_ch.seconds + 10) 
                        continue
                    except Exception as e_ch_proc:
                        logger.error(f"{log_prefix}  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ '{channel_db_obj.title}': {type(e_ch_proc).__name__} - {e_ch_proc}", exc_info=True)
                        continue 
                    
                    total_new_posts_task += channel_new_posts
                    total_updated_posts_info_task += channel_updated_posts_info
                    total_new_comments_collected_task += channel_new_comments
                    
                    if idx < total_channels_to_process - 1: 
                        logger.debug(f"{log_prefix} –ü–∞—É–∑–∞ 1 —Å–µ–∫ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–∞–Ω–∞–ª–∞.")
                        await asyncio.sleep(1) 
                
                await db.commit() 
                
                final_summary = f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ö–∞–Ω–∞–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_channels_count}, –ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤: {total_new_posts_task}, –û–±–Ω–æ–≤–ª–µ–Ω–æ –∏–Ω—Ñ–æ –æ –ø–æ—Å—Ç–∞—Ö: {total_updated_posts_info_task}, –ù–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å–æ–±—Ä–∞–Ω–æ: {total_new_comments_collected_task}."
                logger.info(f"{log_prefix} {final_summary}")
                
                current_meta = {'current_step': '–î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ AI-–∞–Ω–∞–ª–∏–∑—É', 'progress': 85, 'summary_so_far': final_summary}
                self.update_state(state='PROGRESS', meta=current_meta)

                if analyze_new_comments and newly_added_comment_ids_for_ai_task:
                    unique_comment_ids = sorted(list(set(newly_added_comment_ids_for_ai_task)))
                    logger.info(f"{log_prefix} –ó–∞–ø—É—Å–∫ AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(unique_comment_ids)} –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.")
                    
                    ai_analysis_sub_batch_size = settings.COMMENT_ENQUEUE_BATCH_SIZE 
                    
                    num_sub_tasks = 0
                    for i in range(0, len(unique_comment_ids), ai_analysis_sub_batch_size):
                        batch_comment_ids = unique_comment_ids[i:i + ai_analysis_sub_batch_size]
                        logger.info(f"{log_prefix}  –°—Ç–∞–≤–ª—é –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ AI-–∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á—É enqueue_comments_for_ai_feature_analysis_task —Å {len(batch_comment_ids)} ID –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.")
                        enqueue_comments_for_ai_feature_analysis_task.delay(
                            comment_ids_to_process=batch_comment_ids
                        )
                        num_sub_tasks += 1

                    current_meta['current_step'] = f'AI-–∞–Ω–∞–ª–∏–∑ –¥–ª—è {len(unique_comment_ids)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å ({num_sub_tasks} –∑–∞–¥–∞—á(–∏) enqueue_comments_for_ai_feature_analysis_task)'
                    current_meta['progress'] = 95
                    self.update_state(state='PROGRESS', meta=current_meta)

                elif analyze_new_comments:
                    logger.info(f"{log_prefix} –ù–µ—Ç –Ω–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞.")
                    current_meta['current_step'] = '–ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞'
                    current_meta['progress'] = 95
                    self.update_state(state='PROGRESS', meta=current_meta)
                
                self.update_state(state='SUCCESS', meta={'current_step': '–ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!', 'progress': 100, 'result_summary': final_summary})
                return final_summary
        except ConnectionRefusedError as e_auth_tg: 
            logger.error(f"{log_prefix} –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò TELETHON: {e_auth_tg}", exc_info=True)
            self.update_state(state='FAILURE', meta={'current_step': '–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ Telegram', 'error': str(e_auth_tg), 'progress': 100}) 
            raise 
        except Exception as e_main_refresh:
            logger.error(f"{log_prefix} –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_advanced_refresh_logic: {type(e_main_refresh).__name__} - {e_main_refresh}", exc_info=True)
            self.update_state(state='FAILURE', meta={'current_step': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {type(e_main_refresh).__name__}', 'error': str(e_main_refresh), 'progress': 100})
            raise 
        finally:
            if tg_client and tg_client.is_connected():
                logger.info(f"{log_prefix} –û—Ç–∫–ª—é—á–µ–Ω–∏–µ Telegram –∫–ª–∏–µ–Ω—Ç–∞ –≤ finally.")
                try:
                    await tg_client.disconnect()
                except Exception as e_disconnect: 
                    logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ tg_client: {e_disconnect}", exc_info=True)
            if local_engine:
                logger.info(f"{log_prefix} –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ async_engine –≤ finally.")
                await local_engine.dispose()

    try:
        result_message = asyncio.run(_async_advanced_refresh_logic())
        task_duration = time.time() - task_start_time
        logger.info(f"{log_prefix} Celery —Ç–∞—Å–∫ '{self.name}' (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE) –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        return result_message
    except Exception as e_task_level: 
        task_duration = time.time() - task_start_time
        logger.error(f"!!! Celery: –û–®–ò–ë–ö–ê –£–†–û–í–ù–Ø –ó–ê–î–ê–ß–ò –≤ '{self.name}' (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE) (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}", exc_info=True)
        
        current_task_info = self.AsyncResult(self.request.id).info
        is_failure_already_set_specifically = isinstance(current_task_info, dict) and \
                                            self.AsyncResult(self.request.id).state == 'FAILURE' and \
                                            current_task_info.get('error') 

        if not is_failure_already_set_specifically:
             self.update_state(state='FAILURE', meta={'current_step': f'–û–±—â–∞—è –æ—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏: {type(e_task_level).__name__}', 'error': str(e_task_level), 'progress': 100})
        
        if isinstance(e_task_level, ConnectionRefusedError): 
            logger.warning(f"Celery: –ù–ï –ë–£–î–ï–¢ –ü–û–í–¢–û–†–ê –¥–ª—è {self.request.id} –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.")
            raise e_task_level from None 
        
        try:
            if self.request.retries < self.max_retries:
                default_retry_delay_val = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 600
                countdown = int(default_retry_delay_val * (2 ** self.request.retries))
                logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE) —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫ –∏–∑-–∑–∞: {type(e_task_level).__name__}")
                raise self.retry(exc=e_task_level, countdown=countdown)
            else:
                logger.error(f"Celery: Max retries ({self.max_retries}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –¥–ª—è {self.request.id} (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE). –û—à–∏–±–∫–∞: {type(e_task_level).__name__}")
                raise e_task_level from None
        except Exception as e_retry_logic: 
            logger.error(f"Celery: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è {self.request.id} (—Å –õ–û–ö–ê–õ–¨–ù–´–ú ENGINE): {type(e_retry_logic).__name__}", exc_info=True)
            raise e_task_level from e_retry_logic