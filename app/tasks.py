# app/tasks.py

import asyncio
import os
import time
import traceback
import json
from datetime import timezone, datetime, timedelta
from typing import List, Dict, Any, Optional

import openai 
from openai import OpenAIError 

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, aliased
from sqlalchemy.future import select
from sqlalchemy import desc, func, update, cast, literal_column, nullslast, Integer as SAInteger, or_ # –î–æ–±–∞–≤–∏–ª or_
from sqlalchemy.dialects.postgresql import JSONB

import telegram 
from telegram.constants import ParseMode 
from telegram import helpers 

from telethon.errors import (
    FloodWaitError, ChannelPrivateError, UsernameInvalidError, UsernameNotOccupiedError,
    MessageIdInvalidError as TelethonMessageIdInvalidError
)
from telethon.tl.types import (
    Message, User as TelethonUserType, Channel as TelethonChannelType, Chat as TelethonChatType,
    MessageMediaPhoto, MessageMediaDocument, MessageMediaPoll, MessageMediaWebPage,
    MessageMediaGame, MessageMediaInvoice, MessageMediaGeo, MessageMediaContact, MessageMediaDice,
    MessageMediaUnsupported, MessageMediaEmpty, Poll, PollAnswer, ReactionCount, ReactionEmoji, ReactionCustomEmoji,
    MessageReplies, PeerUser, PeerChat, PeerChannel, MessageReplyHeader,
    DocumentAttributeFilename, DocumentAttributeAnimated, DocumentAttributeVideo, DocumentAttributeAudio,
    WebPage, WebPageEmpty, MessageService
)
from telethon import TelegramClient

from app.celery_app import celery_instance
from app.core.config import settings
from app.models.telegram_data import Channel, Post, Comment 

from app.db.session import get_async_session_context_manager 
try:
    from app.services.llm_service import –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm 
except ImportError:
    async def –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm(prompt: str, –º–æ–¥–µ–ª—å: str, **kwargs) -> Optional[str]:
        print(f"–ó–ê–ì–õ–£–®–ö–ê: llm_service.–æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm –≤—ã–∑–≤–∞–Ω —Å –ø—Ä–æ–º—Ç–æ–º: {prompt[:100]}...")
        return None

logger = celery_instance.log.get_default_logger()


@celery_instance.task(name="add")
def add(x, y): logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–∞—Å–∫ 'add': {x} + {y}"); time.sleep(5); result = x + y; logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç 'add': {result}"); return result

@celery_instance.task(name="simple_debug_task")
def simple_debug_task(message: str): logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–∞—Å–∫ 'simple_debug_task' –ø–æ–ª—É—á–∏–ª: {message}"); time.sleep(3); return f"–°–æ–æ–±—â–µ–Ω–∏–µ '{message}' –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ"

async def _process_media_for_db(message_media: Any) -> tuple[Optional[str], Optional[Dict[str, Any]]]:
    media_type_str: Optional[str] = None; media_info_dict: Dict[str, Any] = {}
    if not message_media or isinstance(message_media, MessageMediaEmpty): return None, None
    if isinstance(message_media, MessageMediaPhoto):
        media_type_str = "photo";
        if message_media.photo and hasattr(message_media.photo, 'id'): media_info_dict['id'] = message_media.photo.id
        if hasattr(message_media, 'ttl_seconds') and message_media.ttl_seconds: media_info_dict['ttl_seconds'] = message_media.ttl_seconds
    elif isinstance(message_media, MessageMediaDocument):
        doc = message_media.document; media_type_str = "document"
        if hasattr(doc, 'id'): media_info_dict['id'] = doc.id
        if hasattr(doc, 'mime_type'): media_info_dict['mime_type'] = doc.mime_type
        filename = next((attr.file_name for attr in doc.attributes if isinstance(attr, DocumentAttributeFilename)), None)
        if filename: media_info_dict['filename'] = filename
        duration = next((attr.duration for attr in doc.attributes if isinstance(attr, (DocumentAttributeAudio, DocumentAttributeVideo))), None)
        if duration is not None: media_info_dict['duration'] = float(duration)
        is_gif = any(isinstance(attr, DocumentAttributeAnimated) for attr in doc.attributes); is_video = any(isinstance(attr, DocumentAttributeVideo) and not getattr(attr, 'round_message', False) for attr in doc.attributes); is_audio = any(isinstance(attr, DocumentAttributeAudio) and not getattr(attr, 'voice', False) for attr in doc.attributes); is_voice = any(isinstance(attr, DocumentAttributeAudio) and getattr(attr, 'voice', False) for attr in doc.attributes); is_video_note = any(isinstance(attr, DocumentAttributeVideo) and getattr(attr, 'round_message', False) for attr in doc.attributes)
        if is_gif: media_type_str = "gif"
        elif is_video_note: media_type_str = "video_note"
        elif is_voice: media_type_str = "voice"
        elif is_video: media_type_str = "video"
        elif is_audio: media_type_str = "audio"
    elif isinstance(message_media, MessageMediaPoll):
        media_type_str = "poll"; poll: Poll = message_media.poll
        media_info_dict['question'] = str(poll.question.text) if hasattr(poll.question, 'text') and poll.question.text is not None else str(poll.question)
        media_info_dict['answers'] = [{'text': str(ans.text), 'option': ans.option.decode('utf-8','replace')} for ans in poll.answers]
        media_info_dict['closed'] = poll.closed; media_info_dict['quiz'] = poll.quiz
        if message_media.results and message_media.results.total_voters is not None:
            media_info_dict['total_voters'] = message_media.results.total_voters
            if message_media.results.results: media_info_dict['results_summary'] = [{'option': ans.option.decode('utf-8','replace'), 'voters': ans.voters, 'correct': getattr(ans, 'correct', None)} for ans in message_media.results.results]
    elif isinstance(message_media, MessageMediaWebPage):
        media_type_str = "webpage"; wp: WebPage | WebPageEmpty = message_media.webpage
        if isinstance(wp, WebPage): 
            media_info_dict['url'] = wp.url; media_info_dict['display_url'] = wp.display_url; media_info_dict['type'] = wp.type
            media_info_dict['site_name'] = str(wp.site_name) if wp.site_name else None
            media_info_dict['title'] = str(wp.title) if wp.title else None
            media_info_dict['description'] = str(wp.description) if wp.description else None
            if wp.photo and hasattr(wp.photo, 'id'): media_info_dict['photo_id'] = wp.photo.id
            if wp.duration: media_info_dict['duration'] = float(wp.duration)
    elif isinstance(message_media, MessageMediaGeo) and hasattr(message_media.geo, 'lat') and hasattr(message_media.geo, 'long'): media_type_str = "geo"; media_info_dict['latitude'] = message_media.geo.lat; media_info_dict['longitude'] = message_media.geo.long
    elif isinstance(message_media, MessageMediaContact): media_type_str = "contact"; media_info_dict['phone_number'] = message_media.phone_number; media_info_dict['first_name'] = message_media.first_name; media_info_dict['last_name'] = message_media.last_name
    elif isinstance(message_media, MessageMediaGame) and message_media.game: media_type_str = "game"; media_info_dict['title'] = str(message_media.game.title) if message_media.game.title else None
    elif isinstance(message_media, MessageMediaInvoice) and message_media.title: media_type_str = "invoice"; media_info_dict['title'] = str(message_media.title) if message_media.title else None
    elif isinstance(message_media, MessageMediaDice): media_type_str = "dice"; media_info_dict['emoticon'] = message_media.emoticon; media_info_dict['value'] = message_media.value
    elif isinstance(message_media, MessageMediaUnsupported): media_type_str = "unsupported"
    else:
        if media_type_str is None: media_type_str = f"other_{type(message_media).__name__}"; media_info_dict['raw_type'] = type(message_media).__name__
    return media_type_str, media_info_dict if media_info_dict else None

async def _process_reactions_for_db(message_reactions_attr: Optional[MessageReplies]) -> Optional[List[Dict[str, Any]]]:
    if not message_reactions_attr or not message_reactions_attr.results: return None
    processed_reactions = []
    for reaction_count_obj in message_reactions_attr.results:
        reaction_count_obj: ReactionCount; reaction_val = None
        if isinstance(reaction_count_obj.reaction, ReactionEmoji): reaction_val = reaction_count_obj.reaction.emoticon
        elif isinstance(reaction_count_obj.reaction, ReactionCustomEmoji): reaction_val = f"custom_emoji_{reaction_count_obj.reaction.document_id}"
        if reaction_val: processed_reactions.append({"reaction": reaction_val, "count": reaction_count_obj.count})
    return processed_reactions if processed_reactions else None

@celery_instance.task(name="collect_telegram_data", bind=True, max_retries=3, default_retry_delay=60 * 5)
def collect_telegram_data_task(self):
    task_start_time = time.time(); logger.info(f"–ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}) (–†–ê–°–®–ò–†–ï–ù–ù–´–ô —Å–±–æ—Ä)...")
    api_id_val = settings.TELEGRAM_API_ID; api_hash_val = settings.TELEGRAM_API_HASH; phone_number_val = settings.TELEGRAM_PHONE_NUMBER_FOR_LOGIN
    if not all([api_id_val, api_hash_val, phone_number_val]): logger.error("–û—à–∏–±–∫–∞: Telegram API credentials –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã."); return "Config error"
    session_file_path_in_container = "/app/celery_telegram_session"; logger.info(f"Celery Worker session: {session_file_path_in_container}.session")
    ASYNC_DB_URL_FOR_TASK = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
    async def _async_main_logic_collector():
        tg_client = None; local_async_engine = None; total_channels_processed, total_posts_collected, total_comments_collected = 0,0,0
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL_FOR_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)
            logger.info(f"Celery: –°–æ–∑–¥–∞–Ω–∏–µ TGClient: {session_file_path_in_container}"); tg_client = TelegramClient(session_file_path_in_container, api_id_val, api_hash_val)
            logger.info(f"Celery: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ TG..."); await tg_client.connect()
            if not await tg_client.is_user_authorized(): raise ConnectionRefusedError(f"Celery: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω –¥–ª—è {session_file_path_in_container}.session")
            me = await tg_client.get_me(); logger.info(f"Celery: –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω –∫–∞–∫: {me.first_name} (@{me.username or ''})")
            async with LocalAsyncSessionFactory() as db_session:
                active_channels_from_db: List[Channel] = (await db_session.execute(select(Channel).where(Channel.is_active == True))).scalars().all()
                if not active_channels_from_db: logger.info("Celery: –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤."); return "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤."
                logger.info(f"Celery: –ù–∞–π–¥–µ–Ω–æ {len(active_channels_from_db)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.")
                for channel_db_obj in active_channels_from_db:
                    logger.info(f"\nCelery: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: '{channel_db_obj.title}' (ID: {channel_db_obj.id})"); total_channels_processed += 1
                    newly_added_post_objects_in_session: list[Post] = []
                    try:
                        channel_entity_tg = await tg_client.get_entity(channel_db_obj.id)
                        if not isinstance(channel_entity_tg, TelethonChannelType) or not (getattr(channel_entity_tg, 'broadcast', False) or getattr(channel_entity_tg, 'megagroup', False)):
                            logger.warning(f"  –ö–∞–Ω–∞–ª {channel_db_obj.id} –Ω–µ–≤–∞–ª–∏–¥–µ–Ω. –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º."); channel_db_obj.is_active = False; db_session.add(channel_db_obj); continue
                        iter_messages_params = {"entity": channel_entity_tg, "limit": settings.POST_FETCH_LIMIT}
                        if channel_db_obj.last_processed_post_id: iter_messages_params["min_id"] = channel_db_obj.last_processed_post_id
                        elif settings.INITIAL_POST_FETCH_START_DATETIME: iter_messages_params["offset_date"] = settings.INITIAL_POST_FETCH_START_DATETIME; iter_messages_params["reverse"] = True
                        latest_post_id_seen_this_run = channel_db_obj.last_processed_post_id or 0; collected_for_this_channel_this_run = 0; temp_posts_buffer_for_db_add: list[Post] = []
                        async for message_tg in tg_client.iter_messages(**iter_messages_params):
                            message_tg: Message
                            if isinstance(message_tg, MessageService) or message_tg.action: continue
                            if not (message_tg.text or message_tg.media or message_tg.poll): continue
                            if message_tg.id > latest_post_id_seen_this_run: latest_post_id_seen_this_run = message_tg.id
                            if (await db_session.execute(select(Post.id).where(Post.telegram_post_id == message_tg.id, Post.channel_id == channel_db_obj.id))).scalar_one_or_none() is not None: continue
                            post_text = None; post_caption = None
                            if message_tg.media and message_tg.text: post_caption = message_tg.text
                            elif not message_tg.media and message_tg.text: post_text = message_tg.text
                            media_type, media_info = await _process_media_for_db(message_tg.media); reactions_data = await _process_reactions_for_db(message_tg.reactions)
                            reply_to_post_id = message_tg.reply_to.reply_to_msg_id if message_tg.reply_to and hasattr(message_tg.reply_to, 'reply_to_msg_id') else None
                            sender_id = message_tg.from_id.user_id if isinstance(message_tg.from_id, PeerUser) else None
                            post_link = f"https://t.me/{channel_db_obj.username or f'c/{channel_db_obj.id}'}/{message_tg.id}"
                            new_post_db_obj = Post(telegram_post_id=message_tg.id, channel_id=channel_db_obj.id, link=post_link, text_content=post_text, views_count=message_tg.views, posted_at=message_tg.date.replace(tzinfo=timezone.utc) if message_tg.date else datetime.now(timezone.utc), reactions=reactions_data, media_type=media_type, media_content_info=media_info, caption_text=post_caption, reply_to_telegram_post_id=reply_to_post_id, forwards_count=message_tg.forwards, author_signature=message_tg.post_author, sender_user_id=sender_id, grouped_id=message_tg.grouped_id, edited_at=message_tg.edit_date.replace(tzinfo=timezone.utc) if message_tg.edit_date else None, is_pinned=message_tg.pinned or False)
                            temp_posts_buffer_for_db_add.append(new_post_db_obj); newly_added_post_objects_in_session.append(new_post_db_obj); collected_for_this_channel_this_run += 1; total_posts_collected += 1
                        if temp_posts_buffer_for_db_add: db_session.add_all(temp_posts_buffer_for_db_add); logger.info(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–µ—Å—Å–∏—é {collected_for_this_channel_this_run} –ø–æ—Å—Ç–æ–≤.")
                        if latest_post_id_seen_this_run > (channel_db_obj.last_processed_post_id or 0): channel_db_obj.last_processed_post_id = latest_post_id_seen_this_run; db_session.add(channel_db_obj); logger.info(f"  –û–±–Ω–æ–≤–ª–µ–Ω last_id –¥–ª—è –∫–∞–Ω–∞–ª–∞: {latest_post_id_seen_this_run}")
                        if newly_added_post_objects_in_session:
                            logger.info(f"  Celery: –°–±–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è {len(newly_added_post_objects_in_session)} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤..."); await db_session.flush(); comments_collected_channel_total = 0
                            for post_obj_in_db in newly_added_post_objects_in_session:
                                current_post_comm_count = 0
                                try:
                                    async for comment_msg in tg_client.iter_messages(entity=channel_entity_tg, limit=settings.COMMENT_FETCH_LIMIT, reply_to=post_obj_in_db.telegram_post_id):
                                        comment_msg: Message
                                        if comment_msg.action or not (comment_msg.text or comment_msg.media or comment_msg.poll): continue
                                        if (await db_session.execute(select(Comment.id).where(Comment.telegram_comment_id == comment_msg.id, Comment.post_id == post_obj_in_db.id))).scalar_one_or_none() is not None: continue
                                        comm_text, comm_caption = (None, comment_msg.text) if comment_msg.media and comment_msg.text else (comment_msg.text, None)
                                        comm_media_type, comm_media_info = await _process_media_for_db(comment_msg.media); comm_reactions = await _process_reactions_for_db(comment_msg.reactions)
                                        comm_reply_to_id = comment_msg.reply_to.reply_to_msg_id if comment_msg.reply_to and hasattr(comment_msg.reply_to, 'reply_to_msg_id') else None
                                        comm_user_id = None; comm_user_username = None; comm_user_fullname = None
                                        if isinstance(comment_msg.sender, TelethonUserType): comm_user_id = comment_msg.sender.id; comm_user_username = comment_msg.sender.username; comm_user_fullname = f"{comment_msg.sender.first_name or ''} {comment_msg.sender.last_name or ''}".strip() or None
                                        elif comment_msg.from_id and isinstance(comment_msg.from_id, PeerUser): comm_user_id = comment_msg.from_id.user_id
                                        new_comm_obj = Comment(telegram_comment_id=comment_msg.id, post_id=post_obj_in_db.id, telegram_user_id=comm_user_id, user_username=comm_user_username, user_fullname=comm_user_fullname, text_content=comm_text or "", commented_at=comment_msg.date.replace(tzinfo=timezone.utc) if comment_msg.date else datetime.now(timezone.utc), reactions=comm_reactions, reply_to_telegram_comment_id=comm_reply_to_id, media_type=comm_media_type, media_content_info=comm_media_info, caption_text=comm_caption, edited_at=comment_msg.edit_date.replace(tzinfo=timezone.utc) if comment_msg.edit_date else None)
                                        db_session.add(new_comm_obj); current_post_comm_count +=1; total_comments_collected +=1
                                    if current_post_comm_count > 0: await db_session.execute(update(Post).where(Post.id == post_obj_in_db.id).values(comments_count=(Post.comments_count if Post.comments_count is not None else 0) + current_post_comm_count)); comments_collected_channel_total += current_post_comm_count
                                except TelethonMessageIdInvalidError: logger.warning(f"    Celery: –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –¥–ª—è –ø–æ—Å—Ç–∞ {post_obj_in_db.telegram_post_id} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (TMI).")
                                except FloodWaitError as fwe_comm: logger.warning(f"    !!! Celery: FloodWait –∫–æ–º–º. –ø–æ—Å—Ç–∞ {post_obj_in_db.telegram_post_id}: {fwe_comm.seconds}s"); await asyncio.sleep(fwe_comm.seconds + 5)
                                except Exception as e_comm_loop: logger.error(f"    Celery: –ù–ï–û–ñ–ò–î–ê–ù–ù–ê–Ø –æ—à–∏–±–∫–∞ –∫–æ–º–º. –ø–æ—Å—Ç–∞ {post_obj_in_db.telegram_post_id}: {type(e_comm_loop).__name__} - {e_comm_loop}")
                            if comments_collected_channel_total > 0 : logger.info(f"    –°–æ–±—Ä–∞–Ω–æ {comments_collected_channel_total} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∫–∞–Ω–∞–ª–∞.")
                    except (ChannelPrivateError, UsernameInvalidError, UsernameNotOccupiedError) as e_ch_access: logger.warning(f"  –ö–∞–Ω–∞–ª {channel_db_obj.id} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e_ch_access}. –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º."); channel_db_obj.is_active = False; db_session.add(channel_db_obj)
                    except FloodWaitError as fwe_ch: logger.warning(f"  FloodWait –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db_obj.title}: {fwe_ch.seconds} —Å–µ–∫."); await asyncio.sleep(fwe_ch.seconds + 5)
                    except Exception as e_ch_proc: logger.error(f"  Celery: –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–∞ '{channel_db_obj.title}': {type(e_ch_proc).__name__} - {e_ch_proc}"); traceback.print_exc(limit=1)
                await db_session.commit() ; logger.info(f"\nCelery: –í—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_channels_processed} –∫–∞–Ω–∞–ª–æ–≤.")
            return f"Celery: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {total_channels_processed}ch, {total_posts_collected}p, {total_comments_collected}c."
        except ConnectionRefusedError as e_auth: logger.error(f"!!! Celery: –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò TELETHON: {e_auth}"); raise
        except Exception as e_async_logic: logger.error(f"!!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_main_logic_collector: {type(e_async_logic).__name__} {e_async_logic}"); traceback.print_exc(); raise
        finally:
            if tg_client and tg_client.is_connected(): logger.info("Celery: –û—Ç–∫–ª—é—á–µ–Ω–∏–µ Telegram..."); await tg_client.disconnect()
            if local_async_engine: logger.info("Celery: –ó–∞–∫—Ä—ã—Ç–∏–µ –ë–î..."); await local_async_engine.dispose()
    try:
        result_message = asyncio.run(_async_main_logic_collector())
        task_duration = time.time() - task_start_time; logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except ConnectionRefusedError as e_auth_final: 
        task_duration = time.time() - task_start_time; logger.error(f"!!! Celery: –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {e_auth_final}. –ù–ï –ü–û–í–¢–û–†–Ø–¢–¨."); raise e_auth_final 
    except Exception as e_task_level: 
        task_duration = time.time() - task_start_time; logger.error(f"!!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}"); traceback.print_exc()
        try: 
            retry_delay_seconds = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 300; countdown = int(retry_delay_seconds * (2 ** self.request.retries))
            logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫"); raise self.retry(exc=e_task_level, countdown=countdown)
        except self.MaxRetriesExceededError: logger.error(f"Celery: Max retries –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}. –û—à–∏–±–∫–∞: {e_task_level}"); raise e_task_level 
        except Exception as e_retry_logic: logger.error(f"Celery: –û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ retry: {e_retry_logic}"); raise e_task_level 

@celery_instance.task(name="summarize_top_posts", bind=True, max_retries=2, default_retry_delay=300)
def summarize_top_posts_task(self, hours_ago=48, top_n=5):
    task_start_time = time.time(); logger.info(f"–ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}) (AI –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–ø-{top_n} –ø–æ—Å—Ç–æ–≤ –∑–∞ {hours_ago}—á)...")
    if not settings.OPENAI_API_KEY: logger.error("–û—à–∏–±–∫–∞: OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."); return "Config error OpenAI"
    try: openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
    except Exception as e_init_openai: logger.error(f"–û—à–∏–±–∫–∞ OpenAI init: {e_init_openai}"); raise self.retry(exc=e_init_openai)
    ASYNC_DB_URL_FOR_TASK = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
    async def _async_main_logic_summarizer():
        local_async_engine = None; processed_posts_count = 0
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL_FOR_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)
            async with LocalAsyncSessionFactory() as db_session:
                time_threshold = datetime.now(timezone.utc) - timedelta(hours=hours_ago)
                active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery()
                stmt_posts_to_summarize = (select(Post).join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id).where(Post.posted_at >= time_threshold, Post.summary_text == None).order_by(desc(Post.comments_count)).limit(top_n))
                posts_to_process = (await db_session.execute(stmt_posts_to_summarize)).scalars().all()
                if not posts_to_process: logger.info(f"  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."); return "–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏."
                logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(posts_to_process)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.")
                for post_obj in posts_to_process:
                    text_to_summarize = post_obj.caption_text if post_obj.caption_text else post_obj.text_content
                    if not text_to_summarize or len(text_to_summarize.strip()) < 30 : 
                        logger.info(f"    –ü–æ—Å—Ç ID {post_obj.id} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞/–ø–æ–¥–ø–∏—Å–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º."); continue
                    logger.info(f"    –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Å—Ç–∞ ID {post_obj.id}...")
                    try:
                        summary_prompt = f"–¢–µ–∫—Å—Ç –ø–æ—Å—Ç–∞:\n---\n{text_to_summarize[:4000]}\n---\n–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º) –æ—Å–Ω–æ–≤–Ω–æ–π –º—ã—Å–ª–∏ —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞."
                        completion = await asyncio.to_thread(openai_client.chat.completions.create, model="gpt-3.5-turbo", messages=[{"role": "system", "content": "–¢—ã AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π –∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—é–º–µ —Ç–µ–∫—Å—Ç–æ–≤."}, {"role": "user", "content": summary_prompt}], temperature=0.3, max_tokens=200)
                        summary = completion.choices[0].message.content.strip()
                        if summary: post_obj.summary_text = summary; post_obj.updated_at = datetime.now(timezone.utc); db_session.add(post_obj); processed_posts_count += 1; logger.info(f"      –†–µ–∑—é–º–µ –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id} –ø–æ–ª—É—á–µ–Ω–æ.")
                    except OpenAIError as e_llm: logger.error(f"    !!! –û—à–∏–±–∫–∞ OpenAI API –ø–æ—Å—Ç–∞ ID {post_obj.id}: {e_llm}")
                    except Exception as e_sum: logger.error(f"    !!! –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {e_sum}"); traceback.print_exc(limit=1)
                if processed_posts_count > 0: await db_session.commit(); logger.info(f"  –£—Å–ø–µ—à–Ω–æ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {processed_posts_count} –ø–æ—Å—Ç–æ–≤.")
            return f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_posts_count} –ø–æ—Å—Ç–æ–≤."
        except Exception as e_async_sum: logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_main_logic_summarizer: {e_async_sum}"); traceback.print_exc(); raise
        finally:
            if local_async_engine: await local_async_engine.dispose()
    try:
        result_message = asyncio.run(_async_main_logic_summarizer())
        task_duration = time.time() - task_start_time; logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except Exception as e_task_sum: task_duration = time.time() - task_start_time; logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {e_task_sum}"); traceback.print_exc(); raise self.retry(exc=e_task_sum)

@celery_instance.task(name="send_daily_digest", bind=True, max_retries=3, default_retry_delay=180)
def send_daily_digest_task(self, hours_ago_posts=24, top_n_metrics=3):
    task_start_time = time.time(); logger.info(f"–ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}) (–û—Ç–ø—Ä–∞–≤–∫–∞ –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞)...")
    if not settings.TELEGRAM_BOT_TOKEN or not settings.TELEGRAM_TARGET_CHAT_ID: error_msg = "–û—à–∏–±–∫–∞: TELEGRAM_BOT_TOKEN –∏–ª–∏ TELEGRAM_TARGET_CHAT_ID –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã."; logger.error(error_msg); return error_msg
    async def _async_send_digest_logic():
        bot = telegram.Bot(token=settings.TELEGRAM_BOT_TOKEN)
        ASYNC_DB_URL_FOR_TASK_DIGEST = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
        local_async_engine_digest = None; message_parts = [] 
        try:
            local_async_engine_digest = create_async_engine(ASYNC_DB_URL_FOR_TASK_DIGEST, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactoryDigest = sessionmaker(bind=local_async_engine_digest, class_=AsyncSession, expire_on_commit=False)
            async with LocalAsyncSessionFactoryDigest() as db_session:
                time_threshold_posts = datetime.now(timezone.utc) - timedelta(hours=hours_ago_posts)
                message_parts.append(helpers.escape_markdown(f" digest for Insight-Compass –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours_ago_posts} —á–∞—Å–∞:\n", version=2))
                active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery("active_channels_sq_digest")
                stmt_new_posts_count = (select(func.count(Post.id)).join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id).where(Post.posted_at >= time_threshold_posts))
                new_posts_count = (await db_session.execute(stmt_new_posts_count)).scalar_one_or_none() or 0
                message_parts.append(helpers.escape_markdown(f"üì∞ –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ (–∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤): {new_posts_count}\n", version=2))
                async def get_top_posts_by_metric(metric_column_or_expr_name: str, metric_display_name: str, is_sum_from_jsonb_flag=False):
                    top_posts_list = []; p_alias_name = f"p_digest_{''.join(filter(str.isalnum, metric_display_name.lower()))}"; p_alias = aliased(Post, name=p_alias_name)
                    select_fields = [p_alias.link, p_alias.summary_text, p_alias.post_sentiment_label, Channel.title.label("channel_title")]; stmt_top = select(*select_fields); order_by_col = None
                    if is_sum_from_jsonb_flag:
                        rx_elements_cte_name = f"rx_elements_cte_{''.join(filter(str.isalnum, metric_display_name.lower()))}"; rx_elements_cte = (select(p_alias.id.label("post_id_for_reactions"), cast(func.jsonb_array_elements(p_alias.reactions).op('->>')('count'), SAInteger).label("reaction_item_count")).select_from(p_alias).where(p_alias.reactions.isnot(None)).where(func.jsonb_typeof(p_alias.reactions) == 'array').cte(rx_elements_cte_name))
                        sum_rx_cte_name = f"sum_rx_cte_{''.join(filter(str.isalnum, metric_display_name.lower()))}"; sum_reactions_cte = (select(rx_elements_cte.c.post_id_for_reactions.label("post_id"), func.sum(rx_elements_cte.c.reaction_item_count).label("metric_value_for_digest")).group_by(rx_elements_cte.c.post_id_for_reactions).cte(sum_rx_cte_name))
                        stmt_top = stmt_top.add_columns(sum_reactions_cte.c.metric_value_for_digest).outerjoin(sum_reactions_cte, p_alias.id == sum_reactions_cte.c.post_id); order_by_col = sum_reactions_cte.c.metric_value_for_digest
                    else: actual_metric_column = getattr(p_alias, metric_column_or_expr_name); stmt_top = stmt_top.add_columns(actual_metric_column.label("metric_value_for_digest")); order_by_col = actual_metric_column
                    stmt_top = stmt_top.join(Channel, p_alias.channel_id == Channel.id).where(Channel.is_active == True).where(p_alias.posted_at >= time_threshold_posts).where(p_alias.summary_text.isnot(None)).order_by(order_by_col.desc().nullslast()).limit(top_n_metrics)
                    for row in (await db_session.execute(stmt_top)).all(): top_posts_list.append({"link": row.link, "summary": row.summary_text, "sentiment": row.post_sentiment_label, "channel_title": row.channel_title, "metric_value": row.metric_value_for_digest})
                    return top_posts_list
                tops_to_include = [{"metric_name": "comments_count", "label": "üí¨ –¢–æ–ø –ø–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º", "emoji": "üó£Ô∏è", "unit": "–ö–æ–º–º."}, {"metric_name": "views_count", "label": "üëÄ –¢–æ–ø –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞–º", "emoji": "üëÅÔ∏è", "unit": "–ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤"}, {"metric_name": "reactions", "label": "‚ù§Ô∏è –¢–æ–ø –ø–æ —Å—É–º–º–µ —Ä–µ–∞–∫—Ü–∏–π", "emoji": "üëç", "unit": "–†–µ–∞–∫—Ü–∏–π", "is_sum_jsonb": True}]
                for top_conf in tops_to_include:
                    posts_data = await get_top_posts_by_metric(top_conf["metric_name"], top_conf["label"], is_sum_from_jsonb_flag=top_conf.get("is_sum_jsonb", False))
                    if posts_data:
                        message_parts.append(f"\n{helpers.escape_markdown(top_conf['label'], version=2)} \\(—Å AI\\-—Ä–µ–∑—é–º–µ, —Ç–æ–ø\\-{len(posts_data)}\\):\n")
                        for i, p_data in enumerate(posts_data):
                            link_md = helpers.escape_markdown(p_data["link"] or "#", version=2); summary_md = helpers.escape_markdown(p_data["summary"] or "–†–µ–∑—é–º–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.", version=2); channel_md = helpers.escape_markdown(p_data["channel_title"] or "–ù–µ–∏–∑–≤. –∫–∞–Ω–∞–ª", version=2); metric_val_md = helpers.escape_markdown(str(p_data["metric_value"] or 0), version=2)
                            s_emoji = "üòê " 
                            s_label_text = helpers.escape_markdown("N/A", version=2)
                            if p_data["sentiment"]: 
                                s_label_text = helpers.escape_markdown(p_data["sentiment"].capitalize(), version=2)
                                # --- –ù–ê–ß–ê–õ–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---
                                if p_data["sentiment"] == "positive": 
                                    s_emoji = "üòä "
                                elif p_data["sentiment"] == "negative": 
                                    s_emoji = "üò† "
                                # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---
                            message_parts.append(f"\n{i+1}\\. {channel_md} [{helpers.escape_markdown('–ü–æ—Å—Ç',version=2)}]({link_md})\n   {top_conf['emoji']} {helpers.escape_markdown(top_conf['unit'], version=2)}: {metric_val_md} {s_emoji}{s_label_text}\n   üìù _{summary_md}_\n")
            digest_message_final = "".join(message_parts)
            if len(digest_message_final) > 4096: digest_message_final = digest_message_final[:4090] + helpers.escape_markdown("...", version=2); logger.warning("  –í–ù–ò–ú–ê–ù–ò–ï: –î–∞–π–¥–∂–µ—Å—Ç –±—ã–ª –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ Telegram.")
            logger.info(f"  –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è Telegram (–Ω–∞—á–∞–ª–æ):\n---\n{digest_message_final[:500]}...\n---")
            await bot.send_message(chat_id=settings.TELEGRAM_TARGET_CHAT_ID, text=digest_message_final, parse_mode=ParseMode.MARKDOWN_V2, disable_web_page_preview=True)
            return f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω."
        except Exception as e_digest_logic: logger.error(f"!!! –û—à–∏–±–∫–∞ –≤ _async_send_digest_logic: {e_digest_logic}"); traceback.print_exc(); raise
        finally:
            if local_async_engine_digest: await local_async_engine_digest.dispose()
    try:
        result_message = asyncio.run(_async_send_digest_logic())
        task_duration = time.time() - task_start_time; logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except Exception as e_task_digest: task_duration = time.time() - task_start_time; logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {e_task_digest}"); traceback.print_exc(); raise self.retry(exc=e_task_digest)

@celery_instance.task(name="analyze_posts_sentiment", bind=True, max_retries=2, default_retry_delay=300)
def analyze_posts_sentiment_task(self, limit_posts_to_analyze=10):
    task_start_time = time.time(); logger.info(f"–ó–∞–ø—É—â–µ–Ω Celery —Ç–∞—Å–∫ '{self.name}' (ID: {self.request.id}) (–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –ª–∏–º–∏—Ç: {limit_posts_to_analyze})...")
    if not settings.OPENAI_API_KEY: logger.error("–û—à–∏–±–∫–∞: OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."); return "Config error OpenAI"
    try: openai_client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
    except Exception as e_init_openai_sentiment: logger.error(f"–û—à–∏–±–∫–∞ OpenAI init: {e_init_openai_sentiment}"); raise self.retry(exc=e_init_openai_sentiment)
    ASYNC_DB_URL_FOR_TASK = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
    async def _async_main_logic_sentiment_analyzer():
        local_async_engine = None; analyzed_posts_count = 0
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL_FOR_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)
            async with LocalAsyncSessionFactory() as db_session:
                active_channels_subquery = select(Channel.id).where(Channel.is_active == True).subquery()
                stmt_posts_to_analyze = (select(Post).join(active_channels_subquery, Post.channel_id == active_channels_subquery.c.id).where(or_(Post.text_content.isnot(None), Post.caption_text.isnot(None))).where(Post.post_sentiment_label == None).order_by(Post.posted_at.asc()).limit(limit_posts_to_analyze))
                posts_to_process_result = await db_session.execute(stmt_posts_to_analyze) # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç execute
                posts_to_process = posts_to_process_result.scalars().all()

                if not posts_to_process: logger.info(f"  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏."); return "–ù–µ—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
                logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {len(posts_to_process)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
                for post_obj in posts_to_process:
                    text_for_analysis = post_obj.caption_text if post_obj.caption_text else post_obj.text_content
                    if not text_for_analysis or not text_for_analysis.strip(): logger.info(f"    –ü–æ—Å—Ç ID {post_obj.id} –Ω–µ –∏–º–µ–µ—Ç —Ç–µ–∫—Å—Ç–∞/–ø–æ–¥–ø–∏—Å–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º."); continue
                    logger.info(f"    –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞ ID {post_obj.id}...")
                    s_label, s_score = "neutral", 0.0 
                    try: 
                        prompt = f"–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (JSON: sentiment_label: [positive,negative,neutral,mixed], sentiment_score: [-1.0,1.0]):\n---\n{text_for_analysis[:3500]}\n---\nJSON_RESPONSE:"
                        completion = await asyncio.to_thread(openai_client.chat.completions.create, model="gpt-3.5-turbo", messages=[{"role": "system", "content": "AI-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (JSON)."}, {"role": "user", "content": prompt}], temperature=0.2, max_tokens=60, response_format={"type": "json_object"})
                        raw_resp = completion.choices[0].message.content
                        if raw_resp:
                            try: # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π try –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                                data = json.loads(raw_resp)
                                s_label_candidate = data.get("sentiment_label", "neutral") # –ò—Å–ø–æ–ª—å–∑—É–µ–º _candidate –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
                                s_score_candidate = float(data.get("sentiment_score", 0.0))
                                
                                # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                                if s_label_candidate in ["positive", "negative", "neutral", "mixed"]:
                                    s_label = s_label_candidate
                                else:
                                    logger.warning(f"      –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π sentiment_label '{s_label_candidate}' –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω 'neutral'.")
                                    s_label = "neutral" 
                                
                                if -1.0 <= s_score_candidate <= 1.0:
                                    s_score = s_score_candidate
                                else:
                                    logger.warning(f"      –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: LLM –≤–µ—Ä–Ω—É–ª sentiment_score –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {s_score_candidate} –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω 0.0.")
                                    s_score = 0.0
                            except (json.JSONDecodeError, TypeError, ValueError) as e_json: 
                                logger.error(f"  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç LLM –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id} ({e_json}): {raw_resp}")
                                # s_label –∏ s_score –æ—Å—Ç–∞—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ('neutral', 0.0)
                        else: 
                             logger.warning(f"      OpenAI –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}.")
                            
                    except OpenAIError as e_llm_sentiment: 
                        logger.error(f"    !!! –û—à–∏–±–∫–∞ OpenAI API –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_obj.id}: {e_llm_sentiment}"); 
                        continue 
                    except Exception as e_sa_general: 
                        logger.error(f"    !!! –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–∞ ID {post_obj.id}: {e_sa_general}", exc_info=True); 
                        continue 
                    
                    post_obj.post_sentiment_label = s_label
                    post_obj.post_sentiment_score = s_score
                    post_obj.updated_at = datetime.now(timezone.utc)
                    db_session.add(post_obj)
                    analyzed_posts_count += 1
                    logger.info(f"      –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–∞ ID {post_obj.id}: {s_label} ({s_score:.2f})")

                if analyzed_posts_count > 0: await db_session.commit(); logger.info(f"  –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ {analyzed_posts_count} –ø–æ—Å—Ç–æ–≤.") # –û–±–Ω–æ–≤–ª–µ–Ω –ª–æ–≥
            return f"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {analyzed_posts_count} –ø–æ—Å—Ç–æ–≤."
        except Exception as e_async_sa_main: logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_main_logic_sentiment_analyzer: {e_async_sa_main}", exc_info=True); raise # exc_info=True
        finally:
            if local_async_engine: await local_async_engine.dispose()
    try:
        result_message = asyncio.run(_async_main_logic_sentiment_analyzer())
        task_duration = time.time() - task_start_time; logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except Exception as e_task_sa_main: task_duration = time.time() - task_start_time; logger.error(f"!!! –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {e_task_sa_main}"); traceback.print_exc(); raise self.retry(exc=e_task_sa_main)

@celery_instance.task(name="tasks.analyze_single_comment_ai_features", bind=True, max_retries=2, default_retry_delay=60 * 2)
def analyze_single_comment_ai_features_task(self, comment_id: int):
    task_start_time = time.time(); logger.info(f"[AICommentAnalysis] –ó–∞–ø—É—â–µ–Ω –∞–Ω–∞–ª–∏–∑ –¥–ª—è comment_id: {comment_id} (Task ID: {self.request.id})")
    if not settings.OPENAI_API_KEY: logger.error("[AICommentAnalysis] –û—à–∏–±–∫–∞: OPENAI_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω."); return f"Config error OpenAI for comment_id: {comment_id}"
    ASYNC_DB_URL_FOR_TASK = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")
    async def _async_analyze_comment_logic():
        local_async_engine = None
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL_FOR_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)
            async with LocalAsyncSessionFactory() as db_session:
                comment_stmt = select(Comment).where(Comment.id == comment_id)
                comment_result = await db_session.execute(comment_stmt); comment = comment_result.scalar_one_or_none()
                if not comment: logger.warning(f"[AICommentAnalysis] –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å ID {comment_id} –Ω–µ –Ω–∞–π–¥–µ–Ω."); return f"Comment ID {comment_id} not found."
                text_to_analyze = comment.text_content or ""
                if comment.caption_text: text_to_analyze = f"{text_to_analyze}\n[–ü–æ–¥–ø–∏—Å—å –∫ –º–µ–¥–∏–∞]: {comment.caption_text}".strip()
                if not text_to_analyze:
                    logger.info(f"[AICommentAnalysis] –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ID {comment_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π.")
                    update_stmt_empty = update(Comment).where(Comment.id == comment_id).values(ai_analysis_completed_at=datetime.now(timezone.utc), extracted_topics=[], extracted_problems=[], extracted_questions=[], extracted_suggestions=[])
                    await db_session.execute(update_stmt_empty); await db_session.commit()
                    return f"Comment ID {comment_id} has no text to analyze."
                prompt_template = """–¢—ã ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π AI-–∞–Ω–∞–ª–∏—Ç–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ Telegram. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏ –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–µ–≥–æ —É–∫–∞–∑–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:
"{comment_text}"

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∫–ª—é—á–∞–º–∏:
- "topics": —Å–ø–∏—Å–æ–∫ –∏–∑ 1-5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º –∏–ª–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π, –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["—Ü–µ–Ω–∞", "–¥–æ—Å—Ç–∞–≤–∫–∞", "–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"]). –ï—Å–ª–∏ —Ç–µ–º –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
- "problems": —Å–ø–∏—Å–æ–∫ –∏–∑ 1-3 –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏–ª–∏ –±–æ–ª–µ–π, –≤—ã—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["—Ç–æ–≤–∞—Ä –ø—Ä–∏—à–µ–ª —Å–ª–æ–º–∞–Ω–Ω—ã–π", "–¥–æ–ª–≥–æ –∂–¥–∞—Ç—å –æ—Ç–≤–µ—Ç–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏"]). –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
- "questions": —Å–ø–∏—Å–æ–∫ –∏–∑ 1-3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["–∫–æ–≥–¥–∞ –ø–æ—á–∏–Ω—è—Ç –±–∞–≥?", "–º–æ–∂–Ω–æ –ª–∏ –≤–µ—Ä–Ω—É—Ç—å –¥–µ–Ω—å–≥–∏?"]). –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
- "suggestions": —Å–ø–∏—Å–æ–∫ –∏–∑ 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–ª–∏ –∏–¥–µ–π, –≤—ã—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["–¥–æ–±–∞–≤—å—Ç–µ —Ç–µ–º–Ω—É—é —Ç–µ–º—É", "—Å–¥–µ–ª–∞–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ–Ω—è—Ç–Ω–µ–µ"]). –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.

–í–∞–∂–Ω–æ:
- –ò–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —è–≤–Ω–æ –∏–ª–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ. –ù–µ –¥–æ–¥—É–º—ã–≤–∞–π.
- –°–ø–∏—Å–∫–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ, –µ–º–∫–∏–µ —Ñ—Ä–∞–∑—ã –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
- –ï—Å–ª–∏ –∫–∞–∫–æ–π-—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Ç –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∫–ª—é—á –≤ JSON –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ `[]`.
- –í–µ—Å—å –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º JSON-–æ–±—ä–µ–∫—Ç–æ–º. –°—Ç—Ä–æ–≥–æ JSON. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π –≤–Ω–µ JSON.
"""
                prompt = prompt_template.format(comment_text=text_to_analyze[:settings.LLM_MAX_PROMPT_LENGTH or 3800])
                llm_response_str = await –æ–¥–∏–Ω–æ—á–Ω—ã–π_–∑–∞–ø—Ä–æ—Å_–∫_llm(prompt, –º–æ–¥–µ–ª—å=settings.OPENAI_DEFAULT_MODEL_FOR_TASKS or "gpt-3.5-turbo", —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞=0.2, –º–∞–∫—Å_—Ç–æ–∫–µ–Ω—ã=300)
                if not llm_response_str: logger.error(f"[AICommentAnalysis] –ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è ID {comment_id}."); return f"No LLM response for comment ID {comment_id}."
                try:
                    if llm_response_str.strip().startswith("```json"): llm_response_str = llm_response_str.split("```json", 1)[1].rsplit("```", 1)[0].strip()
                    elif llm_response_str.strip().startswith("```"): llm_response_str = llm_response_str.split("```",1)[1].rsplit("```",1)[0].strip()
                    analysis_data = json.loads(llm_response_str)
                    required_keys = {"topics": list, "problems": list, "questions": list, "suggestions": list}
                    valid_structure = isinstance(analysis_data, dict) and all(key in analysis_data and isinstance(analysis_data[key], req_type) for key, req_type in required_keys.items())
                    if not valid_structure: logger.error(f"[AICommentAnalysis] –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON –æ—Ç LLM –¥–ª—è comment_id {comment_id}. –û—Ç–≤–µ—Ç: {llm_response_str}"); return f"Invalid JSON structure from LLM for comment ID {comment_id}."
                    update_values = {"extracted_topics": analysis_data.get("topics", []), "extracted_problems": analysis_data.get("problems", []), "extracted_questions": analysis_data.get("questions", []), "extracted_suggestions": analysis_data.get("suggestions", []), "ai_analysis_completed_at": datetime.now(timezone.utc)}
                    update_stmt = update(Comment).where(Comment.id == comment_id).values(**update_values)
                    await db_session.execute(update_stmt); await db_session.commit()
                    logger.info(f"[AICommentAnalysis] –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ –æ–±–Ω–æ–≤–ª–µ–Ω –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π ID {comment_id}.")
                    return f"Successfully analyzed comment ID {comment_id}."
                except json.JSONDecodeError: logger.error(f"[AICommentAnalysis] –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –æ—Ç LLM –¥–ª—è comment_id {comment_id}. –û—Ç–≤–µ—Ç: {llm_response_str}"); return f"JSONDecodeError for comment ID {comment_id}."
                except Exception as e_proc: logger.error(f"[AICommentAnalysis] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ LLM –¥–ª—è comment_id {comment_id}: {e_proc}", exc_info=True); return f"Error processing LLM response for comment ID {comment_id}."
        except Exception as e_general: logger.error(f"[AICommentAnalysis] –û–±—â–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è comment_id {comment_id}: {e_general}", exc_info=True); raise
        finally:
            if local_async_engine: await local_async_engine.dispose()
    try:
        result_message = asyncio.run(_async_analyze_comment_logic())
        task_duration = time.time() - task_start_time; logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' (comment_id: {comment_id}) –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}"); return result_message
    except Exception as e_task_level:
        task_duration = time.time() - task_start_time; logger.error(f"!!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (comment_id: {comment_id}) (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}"); traceback.print_exc()
        try:
            retry_delay_seconds = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 120; countdown = int(retry_delay_seconds * (2 ** self.request.retries))
            logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}) —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫"); raise self.retry(exc=e_task_level, countdown=countdown)
        except self.MaxRetriesExceededError: logger.error(f"Celery: Max retries –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}). –û—à–∏–±–∫–∞: {e_task_level}"); raise e_task_level
        except Exception as e_retry_logic: logger.error(f"Celery: –û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ retry –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id} (comment_id: {comment_id}): {e_retry_logic}"); raise e_task_level

@celery_instance.task(name="tasks.enqueue_comments_for_ai_feature_analysis", bind=True)
def enqueue_comments_for_ai_feature_analysis_task(
    self, 
    limit_comments_to_queue: int = 100, 
    older_than_hours: Optional[int] = None, 
    channel_id_filter: Optional[int] = None,
    process_only_recent_hours: Optional[int] = None,
    comment_ids_to_process: Optional[List[int]] = None
):
    task_start_time = time.time()
    log_prefix = "[AICommentQueue]"
    logger.info(f"{log_prefix} –ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏. –õ–∏–º–∏—Ç: {limit_comments_to_queue}, –°—Ç–∞—Ä—à–µ —á–∞—Å–æ–≤: {older_than_hours}, –ö–∞–Ω–∞–ª ID: {channel_id_filter}, –¢–æ–ª—å–∫–æ –∑–∞ X —á–∞—Å–æ–≤: {process_only_recent_hours}, –°–ø–∏—Å–æ–∫ ID: {'–î–∞ (' + str(len(comment_ids_to_process)) + ')' if comment_ids_to_process else '–ù–µ—Ç'} (Task ID: {self.request.id})") # –£–ª—É—á—à–µ–Ω –ª–æ–≥ –¥–ª—è —Å–ø–∏—Å–∫–∞ ID
    
    ASYNC_DB_URL_FOR_TASK = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")

    async def _async_enqueue_logic():
        nonlocal enqueued_count 
        local_async_engine = None
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL_FOR_TASK, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)

            async with LocalAsyncSessionFactory() as db_session:
                comment_ids_to_enqueue: List[int] = []

                if comment_ids_to_process is not None: # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–µ–Ω–Ω–æ –Ω–∞ None
                    logger.info(f"{log_prefix} –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö ID –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {len(comment_ids_to_process)} —à—Ç.")
                    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ ID, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –µ–≥–æ, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                    # (ai_analysis_completed_at IS NULL) –∏ –∏–º–µ—é—Ç —Ç–µ–∫—Å—Ç.
                    # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ID –∏–∑ —Å–ø–∏—Å–∫–∞.
                    if comment_ids_to_process: # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π
                        stmt_filter_ids = select(Comment.id)\
                            .where(Comment.id.in_(comment_ids_to_process))\
                            .where(Comment.text_content.isnot(None))\
                            .where(Comment.text_content != "")\
                            .where(Comment.ai_analysis_completed_at.is_(None)) # –¢–æ–ª—å–∫–æ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                        
                        filtered_ids_result = await db_session.execute(stmt_filter_ids)
                        comment_ids_to_enqueue = filtered_ids_result.scalars().all()
                        logger.info(f"{log_prefix} –ò–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö ID, {len(comment_ids_to_enqueue)} –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ, —Å —Ç–µ–∫—Å—Ç–æ–º).")
                    else: # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ comment_ids_to_process
                        comment_ids_to_enqueue = []
                else:
                    stmt = select(Comment.id).where(Comment.text_content.isnot(None)).where(Comment.text_content != "")
                    
                    if process_only_recent_hours is not None and process_only_recent_hours > 0:
                        time_threshold_recent = datetime.now(timezone.utc) - timedelta(hours=process_only_recent_hours)
                        stmt = stmt.where(Comment.commented_at >= time_threshold_recent)
                        stmt = stmt.where(Comment.ai_analysis_completed_at == None) 
                        stmt = stmt.order_by(Comment.commented_at.desc()) 
                        logger.info(f"{log_prefix} –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ –Ω–µ–¥–∞–≤–Ω–∏—Ö: –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {process_only_recent_hours} —á–∞—Å–æ–≤, —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –Ω–æ–≤—ã–µ.")
                    else:
                        if older_than_hours is not None:
                            time_threshold_older = datetime.now(timezone.utc) - timedelta(hours=older_than_hours)
                            stmt = stmt.where(
                                (Comment.ai_analysis_completed_at == None) | 
                                (Comment.ai_analysis_completed_at < time_threshold_older)
                            )
                        else:
                            stmt = stmt.where(Comment.ai_analysis_completed_at == None) 
                        stmt = stmt.order_by(Comment.commented_at.asc()) 
                        logger.info(f"{log_prefix} –†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—ç–∫–ª–æ–≥–∞: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ.")

                    if channel_id_filter:
                        stmt = stmt.join(Post, Comment.post_id == Post.id).where(Post.channel_id == channel_id_filter)
                    
                    stmt = stmt.limit(limit_comments_to_queue) 
                    
                    comment_ids_result = await db_session.execute(stmt)
                    comment_ids_to_enqueue = comment_ids_result.scalars().all()

                if not comment_ids_to_enqueue:
                    logger.info(f"{log_prefix} –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.")
                    return "No comments found to queue based on criteria."

                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {len(comment_ids_to_enqueue)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å. –ó–∞–ø—É—Å–∫–∞—é –∑–∞–¥–∞—á–∏...")
                for comment_id_to_process in comment_ids_to_enqueue:
                    analyze_single_comment_ai_features_task.delay(comment_id_to_process)
                    enqueued_count += 1 
                
                return f"Successfully enqueued {enqueued_count} comments for AI feature analysis."
        except Exception as e:
            logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: {e}", exc_info=True)
            raise 
        finally:
            if local_async_engine:
                await local_async_engine.dispose()
    
    enqueued_count = 0 
    try:
        result_message = asyncio.run(_async_enqueue_logic())
        task_duration = time.time() - task_start_time
        logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        return result_message
    except Exception as e_task_level:
        task_duration = time.time() - task_start_time
        logger.error(f"!!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}")
        traceback.print_exc()
        raise e_task_level

# --- –ù–ê–ß–ê–õ–û: –ù–û–í–ê–Ø –ó–ê–î–ê–ß–ê –î–õ–Ø –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –û–ë–ù–û–í–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–• ---
from app.schemas.ui_schemas import PostRefreshMode, CommentRefreshMode # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Enum'—ã

@celery_instance.task(name="tasks.advanced_data_refresh", bind=True, max_retries=1, default_retry_delay=60 * 10)
def advanced_data_refresh_task(
    self,
    channel_ids: Optional[List[int]] = None,
    post_refresh_mode_str: str = PostRefreshMode.NEW_ONLY.value, 
    post_refresh_days: Optional[int] = None,
    post_refresh_start_date_iso: Optional[str] = None, 
    post_limit_per_channel: int = 100,
    update_existing_posts_info: bool = False,
    comment_refresh_mode_str: str = CommentRefreshMode.ADD_NEW_TO_EXISTING.value, 
    comment_limit_per_post: int = settings.COMMENT_FETCH_LIMIT,
    analyze_new_comments: bool = True
):
    task_start_time = time.time()
    log_prefix = "[AdvancedRefresh]"
    logger.info(f"{log_prefix} –ó–∞–ø—É—â–µ–Ω–∞ –∑–∞–¥–∞—á–∞. ID: {self.request.id}. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: "
                f"channels={channel_ids}, post_mode='{post_refresh_mode_str}', post_days={post_refresh_days}, "
                f"post_start_date='{post_refresh_start_date_iso}', post_limit={post_limit_per_channel}, "
                f"update_existing={update_existing_posts_info}, comment_mode='{comment_refresh_mode_str}', "
                f"comment_limit={comment_limit_per_post}, analyze={analyze_new_comments}")

    try:
        post_refresh_mode = PostRefreshMode(post_refresh_mode_str)
        comment_refresh_mode = CommentRefreshMode(comment_refresh_mode_str)
        post_refresh_start_date: Optional[datetime] = None
        if post_refresh_start_date_iso:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É ISO –≤ datetime.date, –∑–∞—Ç–µ–º –≤ datetime —Å –Ω–∞—á–∞–ª–æ–º –¥–Ω—è –≤ UTC
            parsed_date = date.fromisoformat(post_refresh_start_date_iso)
            post_refresh_start_date = datetime(parsed_date.year, parsed_date.month, parsed_date.day, tzinfo=timezone.utc)
    except ValueError as e:
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        return f"–û—à–∏–±–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}"

    api_id_val = settings.TELEGRAM_API_ID
    api_hash_val = settings.TELEGRAM_API_HASH
    phone_number_val = settings.TELEGRAM_PHONE_NUMBER_FOR_LOGIN
    if not all([api_id_val, api_hash_val, phone_number_val]):
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞: Telegram API credentials –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã.")
        return "Config error: Telegram API credentials"
    
    session_file_path = "/app/celery_telegram_session" 
    ASYNC_DB_URL = settings.DATABASE_URL_FOR_ALEMBIC.replace("postgresql://", "postgresql+asyncpg://") if settings.DATABASE_URL_FOR_ALEMBIC else settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://")

    newly_added_or_updated_comment_ids: List[int] = []

    async def _async_advanced_refresh_logic():
        nonlocal newly_added_or_updated_comment_ids 
        tg_client = None
        local_async_engine = None
        processed_channels_count = 0
        total_new_posts = 0
        total_updated_posts_info = 0 # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏
        total_new_comments_collected = 0
        
        try:
            local_async_engine = create_async_engine(ASYNC_DB_URL, echo=False, pool_pre_ping=True)
            LocalAsyncSessionFactory = sessionmaker(bind=local_async_engine, class_=AsyncSession, expire_on_commit=False)
            
            logger.info(f"{log_prefix} –°–æ–∑–¥–∞–Ω–∏–µ TGClient: {session_file_path}")
            tg_client = TelegramClient(session_file_path, api_id_val, api_hash_val)
            await tg_client.connect()
            if not await tg_client.is_user_authorized():
                raise ConnectionRefusedError(f"Celery: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω –¥–ª—è {session_file_path}.session")
            me = await tg_client.get_me()
            logger.info(f"{log_prefix} TGClient –ø–æ–¥–∫–ª—é—á–µ–Ω –∫–∞–∫: {me.first_name}")

            async with LocalAsyncSessionFactory() as db:
                channels_to_process_q = select(Channel).where(Channel.is_active == True)
                if channel_ids: # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤
                    if not any(channel_ids): # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç, –Ω–æ –Ω–µ None (–Ω–∞–ø—Ä–∏–º–µ—Ä, [])
                         logger.info(f"{log_prefix} –ü–µ—Ä–µ–¥–∞–Ω –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ ID –∫–∞–Ω–∞–ª–æ–≤. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö.")
                         # –û—Å—Ç–∞–≤–ª—è–µ–º channels_to_process_q –∫–∞–∫ –µ—Å—Ç—å (–≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ)
                    else:
                        channels_to_process_q = channels_to_process_q.where(Channel.id.in_(channel_ids))
                
                channels_result = await db.execute(channels_to_process_q)
                channels_db_list: List[Channel] = channels_result.scalars().all()

                if not channels_db_list:
                    logger.info(f"{log_prefix} –ù–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º ID –∏–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.")
                    return "–ù–µ—Ç –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏."

                logger.info(f"{log_prefix} –ù–∞–π–¥–µ–Ω–æ {len(channels_db_list)} –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")

                for channel_db in channels_db_list:
                    processed_channels_count += 1
                    logger.info(f"{log_prefix} –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: '{channel_db.title}' (ID: {channel_db.id})")
                    posts_requiring_comment_scan: List[Post] = [] # –ü–æ—Å—Ç—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å/—Å–æ–±—Ä–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç—ã
                    
                    try:
                        tg_channel_entity = await tg_client.get_entity(channel_db.id)
                    except Exception as e_entity:
                        logger.warning(f"{log_prefix} –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å entity –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db.id}: {e_entity}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–Ω–∞–ª.")
                        continue

                    iter_params: Dict[str, Any] = {"entity": tg_channel_entity, "limit": None} # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é limit=None –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö
                    
                    if post_refresh_mode == PostRefreshMode.NEW_ONLY:
                        iter_params["limit"] = post_limit_per_channel # –î–ª—è –Ω–æ–≤—ã—Ö —Ç–æ–∂–µ –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å, –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –ª–∏–º–∏—Ç –º–∞–ª
                        if channel_db.last_processed_post_id:
                            iter_params["min_id"] = channel_db.last_processed_post_id
                        logger.info(f"{log_prefix}  –†–µ–∂–∏–º: —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã (min_id: {channel_db.last_processed_post_id}). –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤: {iter_params['limit']}")
                    elif post_refresh_mode == PostRefreshMode.LAST_N_DAYS and post_refresh_days:
                        iter_params["offset_date"] = datetime.now(timezone.utc) - timedelta(days=post_refresh_days)
                        iter_params["reverse"] = True 
                        iter_params["limit"] = post_limit_per_channel
                        logger.info(f"{log_prefix}  –†–µ–∂–∏–º: –ø–æ—Å—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {post_refresh_days} –¥–Ω–µ–π. –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤: {post_limit_per_channel}")
                    elif post_refresh_mode == PostRefreshMode.SINCE_DATE and post_refresh_start_date:
                        iter_params["offset_date"] = post_refresh_start_date
                        iter_params["reverse"] = True
                        iter_params["limit"] = post_limit_per_channel
                        logger.info(f"{log_prefix}  –†–µ–∂–∏–º: –ø–æ—Å—Ç—ã —Å –¥–∞—Ç—ã {post_refresh_start_date_iso}. –õ–∏–º–∏—Ç –ø–æ—Å—Ç–æ–≤: {post_limit_per_channel}")
                    else: 
                        logger.warning(f"{log_prefix}  –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å—Ç–æ–≤. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db.id}.")
                        continue 
                    
                    current_channel_latest_post_id_tg = channel_db.last_processed_post_id or 0

                    async for tg_message in tg_client.iter_messages(**iter_params):
                        tg_message: Message
                        if isinstance(tg_message, MessageService) or tg_message.action: continue
                        if not (tg_message.text or tg_message.media or tg_message.poll): continue

                        if tg_message.id > current_channel_latest_post_id_tg:
                             current_channel_latest_post_id_tg = tg_message.id

                        existing_post_stmt = select(Post).where(Post.telegram_post_id == tg_message.id, Post.channel_id == channel_db.id)
                        existing_post_db = (await db.execute(existing_post_stmt)).scalar_one_or_none()

                        post_text_content, post_caption_text = (None, tg_message.text) if tg_message.media and tg_message.text else (tg_message.text if not tg_message.media else None, None)
                        media_type, media_info = await _process_media_for_db(tg_message.media)
                        reactions_data = await _process_reactions_for_db(tg_message.reactions)
                        reply_to_id = tg_message.reply_to.reply_to_msg_id if tg_message.reply_to and hasattr(tg_message.reply_to, 'reply_to_msg_id') else None
                        sender_id_val = tg_message.from_id.user_id if isinstance(tg_message.from_id, PeerUser) else None
                        link_val = f"https://t.me/{channel_db.username or f'c/{channel_db.id}'}/{tg_message.id}"
                        posted_at_val = tg_message.date.replace(tzinfo=timezone.utc) if tg_message.date else datetime.now(timezone.utc)
                        edited_at_val = tg_message.edit_date.replace(tzinfo=timezone.utc) if tg_message.edit_date else None
                        
                        post_for_comments = None
                        if not existing_post_db:
                            logger.debug(f"{log_prefix}    –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Å—Ç–∞ ID {tg_message.id} –∏–∑ –∫–∞–Ω–∞–ª–∞ {channel_db.id}")
                            new_post = Post(
                                telegram_post_id=tg_message.id, channel_id=channel_db.id, link=link_val,
                                text_content=post_text_content, caption_text=post_caption_text,
                                views_count=tg_message.views, comments_count=tg_message.replies.replies if tg_message.replies else 0,
                                posted_at=posted_at_val, reactions=reactions_data, media_type=media_type, 
                                media_content_info=media_info, reply_to_telegram_post_id=reply_to_id, 
                                forwards_count=tg_message.forwards, author_signature=tg_message.post_author, 
                                sender_user_id=sender_id_val, grouped_id=tg_message.grouped_id, 
                                edited_at=edited_at_val, is_pinned=tg_message.pinned or False
                            )
                            db.add(new_post)
                            await db.flush() 
                            post_for_comments = new_post
                            total_new_posts +=1
                        elif update_existing_posts_info:
                            logger.debug(f"{log_prefix}    –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ—Å—Ç–∞ ID {existing_post_db.id} (TG ID: {tg_message.id})")
                            existing_post_db.views_count = tg_message.views
                            existing_post_db.reactions = reactions_data
                            existing_post_db.forwards_count = tg_message.forwards
                            existing_post_db.edited_at = edited_at_val
                            # –û–±–Ω–æ–≤–ª—è–µ–º comments_count –∏–∑ Telegram, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
                            if tg_message.replies and tg_message.replies.replies is not None:
                                existing_post_db.comments_count = tg_message.replies.replies
                            db.add(existing_post_db)
                            post_for_comments = existing_post_db
                            total_updated_posts_info += 1
                        else: # –ü–æ—Å—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, info –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º, –Ω–æ –∫–æ–º–º–µ–Ω—Ç—ã –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
                             post_for_comments = existing_post_db
                        
                        if post_for_comments and comment_refresh_mode != CommentRefreshMode.NEW_POSTS_ONLY:
                            posts_requiring_comment_scan.append(post_for_comments)
                        elif post_for_comments and comment_refresh_mode == CommentRefreshMode.NEW_POSTS_ONLY and not existing_post_db: # –¢–æ–ª—å–∫–æ –¥–ª—è –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
                            posts_requiring_comment_scan.append(post_for_comments)


                    if post_refresh_mode == PostRefreshMode.NEW_ONLY and current_channel_latest_post_id_tg > (channel_db.last_processed_post_id or 0):
                        channel_db.last_processed_post_id = current_channel_latest_post_id_tg
                        db.add(channel_db)
                        logger.info(f"{log_prefix}  –û–±–Ω–æ–≤–ª–µ–Ω last_id –¥–ª—è –∫–∞–Ω–∞–ª–∞ {channel_db.id}: {current_channel_latest_post_id_tg}")

                    # –°–±–æ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
                    if posts_requiring_comment_scan:
                        logger.info(f"{log_prefix}  –°–±–æ—Ä/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è {len(posts_requiring_comment_scan)} –ø–æ—Å—Ç–æ–≤ –∫–∞–Ω–∞–ª–∞ {channel_db.id} (—Ä–µ–∂–∏–º: {comment_refresh_mode.value})...")
                        for post_db_obj in posts_requiring_comment_scan:
                            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É –¥–ª—è comment_refresh_mode
                            # –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è "ADD_NEW_TO_EXISTING" - –ø–æ–ª—É—á–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ –∫–æ–º–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç.
                            # –î–ª—è MVP —Ç–µ–∫—É—â–µ–π "–∫—Ä—É—Ç–æ–π" –∫–Ω–æ–ø–∫–∏, –ø—Ä–æ—Å—Ç–æ–π —Å–±–æ—Ä –Ω–æ–≤—ã—Ö –±—É–¥–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω.
                            
                            # –ü–æ–ª—É—á–∞–µ–º ID —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Å—Ç–∞
                            existing_comment_tg_ids_stmt = select(Comment.telegram_comment_id).where(Comment.post_id == post_db_obj.id)
                            existing_comment_tg_ids_res = await db.execute(existing_comment_tg_ids_stmt)
                            existing_comment_tg_ids_set = set(existing_comment_tg_ids_res.scalars().all())
                            
                            num_new_comments_for_post = 0
                            try:
                                async for tg_comment_msg in tg_client.iter_messages(
                                    entity=tg_channel_entity, 
                                    limit=comment_limit_per_post, 
                                    reply_to=post_db_obj.telegram_post_id
                                ):
                                    tg_comment_msg: Message
                                    if tg_comment_msg.action or not (tg_comment_msg.text or tg_comment_msg.media or tg_comment_msg.poll): continue
                                    
                                    if tg_comment_msg.id in existing_comment_tg_ids_set:
                                        # TODO: –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                        continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π

                                    comm_text, comm_caption = (None, tg_comment_msg.text) if tg_comment_msg.media and tg_comment_msg.text else (tg_comment_msg.text, None)
                                    comm_media_type, comm_media_info = await _process_media_for_db(tg_comment_msg.media)
                                    comm_reactions = await _process_reactions_for_db(tg_comment_msg.reactions)
                                    comm_reply_to_id = tg_comment_msg.reply_to.reply_to_msg_id if tg_comment_msg.reply_to and hasattr(tg_comment_msg.reply_to, 'reply_to_msg_id') else None
                                    comm_user_id, comm_user_username, comm_user_fullname = (None, None, None)
                                    if isinstance(tg_comment_msg.sender, TelethonUserType):
                                        comm_user_id = tg_comment_msg.sender.id; comm_user_username = tg_comment_msg.sender.username
                                        comm_user_fullname = f"{tg_comment_msg.sender.first_name or ''} {tg_comment_msg.sender.last_name or ''}".strip() or None
                                    elif tg_comment_msg.from_id and isinstance(tg_comment_msg.from_id, PeerUser):
                                        comm_user_id = tg_comment_msg.from_id.user_id
                                    
                                    new_comment_db = Comment(
                                        telegram_comment_id=tg_comment_msg.id, post_id=post_db_obj.id,
                                        telegram_user_id=comm_user_id, user_username=comm_user_username, user_fullname=comm_user_fullname,
                                        text_content=comm_text or (comm_caption if not comm_text else ""), 
                                        commented_at=tg_comment_msg.date.replace(tzinfo=timezone.utc) if tg_comment_msg.date else datetime.now(timezone.utc),
                                        reactions=comm_reactions, reply_to_telegram_comment_id=comm_reply_to_id,
                                        media_type=comm_media_type, media_content_info=comm_media_info,
                                        caption_text=comm_caption, 
                                        edited_at=tg_comment_msg.edit_date.replace(tzinfo=timezone.utc) if tg_comment_msg.edit_date else None
                                    )
                                    db.add(new_comment_db)
                                    await db.flush() # –ü–æ–ª—É—á–∞–µ–º ID –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                                    if new_comment_db.id: # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ ID –ø—Ä–∏—Å–≤–æ–µ–Ω
                                        newly_added_or_updated_comment_ids.append(new_comment_db.id)
                                    num_new_comments_for_post += 1
                                    total_new_comments_collected += 1
                                
                                if num_new_comments_for_post > 0:
                                    post_db_obj.comments_count = (post_db_obj.comments_count or 0) + num_new_comments_for_post
                                    db.add(post_db_obj)
                                    logger.info(f"{log_prefix}    –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ {num_new_comments_for_post} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –ø–æ—Å—Ç–∞ ID {post_db_obj.id}")
                            except FloodWaitError as fwe_c:
                                logger.warning(f"{log_prefix}    FloodWait –ø—Ä–∏ —Å–±–æ—Ä–µ –∫–æ–º–º. –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.id}: {fwe_c.seconds}s. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç.")
                                await asyncio.sleep(fwe_c.seconds + 10) 
                                break 
                            except Exception as e_c:
                                logger.error(f"{log_prefix}    –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∫–æ–º–º. –¥–ª—è –ø–æ—Å—Ç–∞ {post_db_obj.id}: {e_c}", exc_info=True)

                await db.commit()
                logger.info(f"{log_prefix} –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {processed_channels_count} –∫–∞–Ω–∞–ª–æ–≤. "
                            f"–ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤: {total_new_posts}, –û–±–Ω–æ–≤–ª–µ–Ω–æ –∏–Ω—Ñ–æ –æ –ø–æ—Å—Ç–∞—Ö: {total_updated_posts_info}, "
                            f"–ù–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {total_new_comments_collected}.")

                if analyze_new_comments and newly_added_or_updated_comment_ids:
                    unique_comment_ids = sorted(list(set(newly_added_or_updated_comment_ids))) # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –µ—Å–ª–∏ –≤–¥—Ä—É–≥
                    logger.info(f"{log_prefix} –ó–∞–ø—É—Å–∫ AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {len(unique_comment_ids)} –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.")
                    enqueue_comments_for_ai_feature_analysis_task.delay(
                        comment_ids_to_process=unique_comment_ids,
                        # limit_comments_to_queue –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤ len(unique_comment_ids), —á—Ç–æ–±—ã –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ
                        limit_comments_to_queue=settings.AI_ANALYSIS_BATCH_SIZE or 100 # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞—Ç—á–∏
                    )
                elif analyze_new_comments:
                    logger.info(f"{log_prefix} –ù–µ—Ç –Ω–æ–≤—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞.")

            return f"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤: {total_new_posts}, –û–±–Ω–æ–≤–ª–µ–Ω–æ –∏–Ω—Ñ–æ: {total_updated_posts_info}, –ù–æ–≤—ã—Ö –∫–æ–º–º.: {total_new_comments_collected}."

        except Exception as e_main_refresh:
            logger.error(f"{log_prefix} –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ _async_advanced_refresh_logic: {e_main_refresh}", exc_info=True)
            raise 
        finally:
            if tg_client and tg_client.is_connected(): await tg_client.disconnect()
            if local_async_engine: await local_async_engine.dispose()
    
    try:
        result_message = asyncio.run(_async_advanced_refresh_logic())
        task_duration = time.time() - task_start_time
        logger.info(f"Celery —Ç–∞—Å–∫ '{self.name}' –£–°–ü–ï–®–ù–û –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {task_duration:.2f} —Å–µ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç: {result_message}")
        return result_message
    except Exception as e_task_level:
        task_duration = time.time() - task_start_time
        logger.error(f"!!! Celery: –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–∞—Å–∫–µ '{self.name}' (–∑–∞ {task_duration:.2f} —Å–µ–∫): {type(e_task_level).__name__} {e_task_level}")
        traceback.print_exc()
        try:
            retry_delay_seconds = self.default_retry_delay if isinstance(self.default_retry_delay, (int, float)) else 600
            countdown = int(retry_delay_seconds * (2 ** self.request.retries))
            logger.info(f"Celery: Retry ({self.request.retries + 1}/{self.max_retries}) —Ç–∞—Å–∫–∞ {self.request.id} —á–µ—Ä–µ–∑ {countdown} —Å–µ–∫")
            raise self.retry(exc=e_task_level, countdown=countdown)
        except self.MaxRetriesExceededError:
            logger.error(f"Celery: Max retries –¥–ª—è —Ç–∞—Å–∫–∞ {self.request.id}. –û—à–∏–±–∫–∞: {e_task_level}")
            raise e_task_level
        except Exception as e_retry_logic:
            logger.error(f"Celery: –û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ retry: {e_retry_logic}")
            raise e_task_level
# --- –ö–û–ù–ï–¶: –ù–û–í–ê–Ø –ó–ê–î–ê–ß–ê –î–õ–Ø –ü–†–û–î–í–ò–ù–£–¢–û–ì–û –û–ë–ù–û–í–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–• ---